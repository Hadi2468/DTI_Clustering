{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"    Version control\\n------------------------\")\n",
    "import os, fnmatch, random, math, sys, datetime\n",
    "from pathlib import Path\n",
    "import numpy as np;              print(\"Numpy\\t\\t\", np.__version__)\n",
    "import matplotlib as mpl;        print(\"matplotlib\\t\", mpl.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib;           print(\"NiBabel\\t\\t {}\".format(nib.__version__))\n",
    "from nibabel.testing import data_path\n",
    "import pandas as pd;             print(\"Pandas\\t\\t {}\".format(pd.__version__))\n",
    "import imageio;                  print(\"imageio\\t\\t {}\".format(imageio.__version__))\n",
    "import h5py;                     print(\"H5py\\t\\t {}\".format(h5py.__version__))\n",
    "import sklearn;                  print(\"Scikit-learn\\t {}\".format(sklearn.__version__))\n",
    "import skimage;                  print(\"Scikit-image\\t {}\".format(skimage.__version__))\n",
    "import tensorflow as tf;         print(\"TensorFlow\\t {}\".format(tf.__version__))\n",
    "import keras as K;               print(\"Keras\\t\\t {}\".format(K.__version__))\n",
    "from tensorflow.keras import models, Input, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, BatchNormalization, Conv3D, MaxPooling3D, UpSampling3D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.initializers import *\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load train data\n",
    "sample_train_subset = np.loadtxt(\"sample_train_1.csv\", dtype=str, delimiter=\",\")\n",
    "train_data = np.load('train_1.npy').reshape(100,182,218,182,1)\n",
    "print('train_data shape is {}'.format(train_data.shape))\n",
    "\n",
    "## Load validation data\n",
    "sample_val_subset = np.loadtxt(\"sample_valid_1.csv\", dtype=str, delimiter=\",\")\n",
    "valid_data = np.load('valid_1.npy').reshape(24,182,218,182,1)\n",
    "print('valid_data shape is {}'.format(valid_data.shape))\n",
    "\n",
    "## Load last weights\n",
    "# last_weights = str(Path(os.path.join(os.getcwd(), \"Weights\")) / \"w_1.hdf5\")\n",
    "# print(last_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters:\n",
    "\n",
    "IMAGE_HEIGHT = train_data.shape[1]\n",
    "IMAGE_WIDTH = train_data.shape[2]\n",
    "IMAGE_DEPTH = train_data.shape[3]\n",
    "batch_size = 1\n",
    "data_shape = [1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "input_shape = [batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "print(\"input-layer shape:\", input_shape)\n",
    "\n",
    "## Encoder\n",
    "input_img = Input(shape=(182, 218, 182, 1), name='Input')\n",
    "x = Conv3D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv1')(input_img)\n",
    "x = BatchNormalization(name='BN_Conv1')(x)\n",
    "x = Conv3D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv2')(x)\n",
    "x = BatchNormalization(name='BN_Conv2')(x)\n",
    "x = Conv3D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv3')(x)\n",
    "x = BatchNormalization(name='BN_Conv3')(x)\n",
    "x = Conv3D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv4')(x)\n",
    "x = BatchNormalization(name='BN_Conv4')(x)\n",
    "x = Conv3D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv5')(x)\n",
    "x = BatchNormalization(name='BN_Conv5')(x)\n",
    "x = Conv3D(filters=4, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv6')(x)\n",
    "x = BatchNormalization(name='BN_Conv6')(x)\n",
    "\n",
    "## Latent Features\n",
    "shape_before_flattening = tf.keras.backend.int_shape(x)\n",
    "x = Flatten(name='Flat')(x)\n",
    "# init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n",
    "# encoded = Dense(50, kernel_initializer=init, activation='relu', name='encoded')(x)\n",
    "# encoded = Dense(50, activation='relu', name='encoded')(x)\n",
    "encoded = x\n",
    "# x = BatchNormalization()(encoded)\n",
    "# x = Dense(np.prod(shape_before_flattening[1:]), activation='relu', kernel_initializer=init)(encoded)\n",
    "# x = Dense(np.prod(shape_before_flattening[1:]), activation='relu')(encoded)\n",
    "x = Reshape(shape_before_flattening[1:], name='UnFlat')(x)\n",
    "\n",
    "## Decoder\n",
    "x = Conv3D(filters=4, kernel_size=3, padding='same', activation='relu', name='DeConv1')(x)\n",
    "x = BatchNormalization(name='BN_DeConv1')(x)\n",
    "x = UpSampling3D(size=(2, 2, 2), name='UpSampling1')(x)\n",
    "x = Conv3D(filters=8, kernel_size=(1,2,1), padding='valid', activation='relu', name='DeConv2')(x)\n",
    "x = BatchNormalization(name='BN_DeConv2')(x)\n",
    "x = UpSampling3D(size=(2, 2, 2), name='UpSampling2')(x)\n",
    "x = Conv3D(filters=16, kernel_size=3, padding='same', activation='relu', name='DeConv3')(x)\n",
    "x = BatchNormalization(name='BN_DeConv3')(x)\n",
    "x = UpSampling3D(size=(2, 2, 2), name='UpSampling3')(x)\n",
    "x = Conv3D(filters=32, kernel_size=(2,1,2), padding='valid', activation='relu', name='DeConv4')(x)\n",
    "x = BatchNormalization(name='BN_DeConv4')(x)\n",
    "x = UpSampling3D(size=(2, 2, 2), name='UpSampling4')(x)\n",
    "x = Conv3D(filters=64, kernel_size=(1,2,1), padding='valid', activation='relu', name='DeConv5')(x)\n",
    "x = BatchNormalization(name='BN_DeConv5')(x)\n",
    "x = UpSampling3D(size=(2, 2, 2), name='UpSampling5')(x)\n",
    "x = Conv3D(filters=128, kernel_size=2, padding='valid', activation='relu', name='DeConv6')(x)\n",
    "x = BatchNormalization(name='BN_DeConv6')(x)\n",
    "x = UpSampling3D(size=(2, 2, 2), name='UpSampling6')(x)\n",
    "decoded = Conv3D(filters=1, kernel_size=3, padding='same', activation='sigmoid', name='Output')(x)\n",
    "\n",
    "model_CAE = Model(inputs=input_img, outputs=decoded)\n",
    "## optimizer=rmsprop, sgd\n",
    "model_CAE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model_CAE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start time:\n",
    "from datetime import datetime\n",
    "start_time = datetime.now().strftime(\"%Y.%m.%d___%H:%M\")\n",
    "print(\"\\nStart Time =\", start_time, \"\\n\")\n",
    "\n",
    "## Loading last weights\n",
    "# model_CAE.load_weights(os.path.join(\"Weights/Weights_L100___2020_12_30___03_43_23.hdf5\"))\n",
    "\n",
    "## Model Fit\n",
    "tb_callback = TensorBoard(os.path.join(\"Logs/L100___\" + start_time), histogram_freq=1)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\n",
    "model_CAE.fit(train_data, train_data, validation_data=(valid_data, valid_data), epochs=1, batch_size=batch_size, shuffle=True, callbacks=[tb_callback], verbose=1)\n",
    "model_CAE.save_weights(os.path.join(\"Weights/L100___\" + start_time + \".hdf5\"))\n",
    "\n",
    "## End time:\n",
    "from datetime import datetime\n",
    "end_time = datetime.now().strftime(\"%Y_%m_%d____%H_%M\")\n",
    "print(\"\\nEnd Time =\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = train_data[0,:].reshape(1, 182, 218, 182, 1)\n",
    "reconstructed = model_CAE.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntrain_data[0,100,100:105,100]\\n\\n {}'.format(train_data[0,100,100:105,100]),'\\n')\n",
    "print('\\nReconstructed_data[0,100,100:105,100]\\n\\n {}'.format(reconstructed[0,100,100:105,100]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = h5_file[list(h5_file.keys())[0]]\n",
    "W = L[list(h5_file.keys())[0]]['kernel:0']\n",
    "print('\\nWeights shape: {}'.format(W.shape))\n",
    "print('\\nWeights[1][1][1]: {}'.format(W[1][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5_file = h5py.File(os.path.join(\"Weights/L100___\" + start_time + \".hdf5\"), 'r')\n",
    "h5_file = h5py.File(os.path.join(\"Weights/w_Pegasus_128.hdf5\"), 'r')\n",
    "Layer_size = len(list(h5_file.keys()))\n",
    "Layer_names = list(h5_file.keys())\n",
    "print(\"There are\", Layer_size, \"layers in this model as:\\n\\n\", Layer_names,'\\n')\n",
    "\n",
    "for l in range(5):  #Layer_size\n",
    "    print('==========================================================\\n')\n",
    "    layers = h5_file[Layer_names[l]]\n",
    "#     print(\"Layer\", l+1, \"-----\", layers)\n",
    "    W = layers[Layer_names[l]]['kernel:0']\n",
    "    print('Layer', l+1, ':', list(h5_file.keys())[l], '\\tWeights\\' shape: {}'.format(W.shape), '\\n')\n",
    "#     print('\\nWeights[1][1][1]: {}'.format(W[1][1][1]))\n",
    "    \n",
    "    Kernel_1 = W.shape[0]\n",
    "    Kernel_2 = W.shape[1]\n",
    "    Kernel_3 = W.shape[2]\n",
    "    Kernel_all = np.zeros([Kernel_1, Kernel_2, Kernel_3])\n",
    "    for f in range(2):   # W.shape[4]\n",
    "        for x in range(Kernel_1):\n",
    "            for y in range(Kernel_2):\n",
    "                for z in range(Kernel_3):\n",
    "                    Kernel_all[x][y][z] = (W[x][y][z])[0][0]\n",
    "        print('\\nWeights of kernel', f+1, 'of', W.shape[4], ':\\n\\n', Kernel_all)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
