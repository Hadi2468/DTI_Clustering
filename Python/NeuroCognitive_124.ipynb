{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import some helpful libraries\n",
    "# print(\"    Version control\\n------------------------\")\n",
    "# import os, fnmatch, random, math, sys, datetime\n",
    "# from pathlib import Path\n",
    "# import numpy as np;              print(\"Numpy\\t\\t\", np.__version__)\n",
    "# import matplotlib as mpl;        print(\"matplotlib\\t\", mpl.__version__)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd;             print(\"Pandas\\t\\t {}\".format(pd.__version__))\n",
    "# from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuroCognitive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Loading original Neurocognitive score data\n",
    "\n",
    "# score_df = pd.read_excel('./Data/Neurocognitive_Scores/Scores_original.xlsx')\n",
    "# df = score_df\n",
    "# for p in range (len(df)):\n",
    "#     if df.ID[p][5] == '_': \n",
    "#         df.ID[p] = df.ID[p][0:4] + '00' + df.ID[p][4:]\n",
    "#     elif df.ID[p][6] == '_': \n",
    "#         df.ID[p] = df.ID[p][0:4] + '0' + df.ID[p][4:]\n",
    "# print('\\033[1m There are totally {} records\\n'.format(len(df)))\n",
    "\n",
    "# ## Extract the Baseline_Phase and save\n",
    "# df = df.rename(columns={'Phase Description':'Phase_Description', 'GIA SS':'GIA_SS', 'Proc Spd SS':'Proc_Spd_SS', 'Working Mem SS':'Working_Mem_SS', 'Broad Attn SS':'Broad_Attn_SS'})\n",
    "# df.index = df[df.columns[0]]\n",
    "# df = df.drop(['ID'], axis=1)\n",
    "# df = df.drop(['Date'], axis=1)\n",
    "# df_base = df[(df.Phase_Description == 'Baseline phase') | (df.Phase_Description == 'Baseline Phase')]\n",
    "# df_base.Phase_Description = 'baseline_phase'\n",
    "# print('\\033[1m There are {} records of baseline_phasse\\n'.format(len(df_base)))\n",
    "# df_base.to_csv((r'./Data/Neurocognitive_Scores/baseline.csv'))\n",
    "\n",
    "# ## Extract the 36_months_Phase and save\n",
    "# df_36 = df[(df.Phase_Description == 'Post Dx 36 months')]# | (df.Phase_Description == 'Baseline Phase')]\n",
    "# df_36.Phase_Description = '36_months'\n",
    "# print('\\033[1m There are {} records of 36_months\\n'.format(len(df_36)))\n",
    "# df_36.to_csv((r'./Data/Neurocognitive_Scores/m36.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generating Scores for Baseline\n",
    "\n",
    "# df_base = pd.read_csv('./Data/Neurocognitive_Scores/baseline.csv')\n",
    "# df_base.index = df_base.ID\n",
    "\n",
    "# ## Baseline_phasse & GIA_Score\n",
    "# df_base_GIA = df_base.GIA_SS\n",
    "# df_base_GIA = df_base_GIA.dropna()   # remove NaN\n",
    "# df_base_GIA = df_base_GIA.astype(int)\n",
    "# # df_base_GIA.to_csv((r'./Data/Neurocognitive_Scores/baseline_GIA.csv'))\n",
    "\n",
    "# ## Baseline_phasse & Process_Spead_Score\n",
    "# df_base_Proc_Spd = df_base.Proc_Spd_SS\n",
    "# df_base_Proc_Spd = df_base_Proc_Spd.dropna()   # remove NaN\n",
    "# df_base_Proc_Spd = df_base_Proc_Spd.astype(int)\n",
    "# # df_base_Proc_Spd.to_csv((r'./Data/Neurocognitive_Scores/baseline_Proc_Spd.csv'))\n",
    "\n",
    "# ## Baseline_phasse & Working_Memory_Score\n",
    "# df_base_Working_Mem = df_base.Working_Mem_SS\n",
    "# df_base_Working_Mem = df_base_Working_Mem.dropna()   # remove NaN\n",
    "# df_base_Working_Mem = df_base_Working_Mem.astype(int)\n",
    "# # df_base_Working_Mem.to_csv((r'./Data/Neurocognitive_Scores/baseline_Working_Mem.csv'))\n",
    "\n",
    "# ## Baseline_phasse & Broad_Attention_Score\n",
    "# df_base_Broad_Attn = df_base.Broad_Attn_SS\n",
    "# df_base_Broad_Attn = df_base_Broad_Attn.dropna()   # remove NaN\n",
    "# df_base_Broad_Attn = df_base_Broad_Attn.astype(int)\n",
    "# # df_base_Broad_Attn.to_csv((r'./Data/Neurocognitive_Scores/baseline_Broad_Attn.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generating Scores for 36_Months\n",
    "\n",
    "# df_m36 = pd.read_csv('./Data/Neurocognitive_Scores/m36.csv')\n",
    "# df_m36.index = df_m36.ID\n",
    "\n",
    "# ## Month-36 & GIA_Score\n",
    "# df_m36_GIA = df_m36.GIA_SS\n",
    "# df_m36_GIA = df_m36_GIA.dropna()   # remove NaN\n",
    "# df_m36_GIA = df_m36_GIA.astype(int)\n",
    "# # df_m36_GIA.to_csv((r'./Data/Neurocognitive_Scores/m36_GIA.csv'))\n",
    "\n",
    "# ## Month-36 & Process_Spead_Score\n",
    "# df_m36_Proc_Spd = df_m36.Proc_Spd_SS\n",
    "# df_m36_Proc_Spd = df_m36_Proc_Spd.dropna()   # remove NaN\n",
    "# df_m36_Proc_Spd = df_m36_Proc_Spd.astype(int)\n",
    "# # df_m36_Proc_Spd.to_csv((r'./Data/Neurocognitive_Scores/m36_Proc_Spd.csv'))\n",
    "\n",
    "# ## Month-36 & Working_Memory_Score\n",
    "# df_m36_Working_Mem = df_m36.Working_Mem_SS\n",
    "# df_m36_Working_Mem = df_m36_Working_Mem.dropna()   # remove NaN\n",
    "# df_m36_Working_Mem = df_m36_Working_Mem.astype(int)\n",
    "# # df_m36_Working_Mem.to_csv((r'./Data/Neurocognitive_Scores/m36_Working_Mem.csv'))\n",
    "\n",
    "# ## Month-36 & Broad_Attention_Score\n",
    "# df_m36_Broad_Attn = df_m36.Broad_Attn_SS\n",
    "# df_m36_Broad_Attn = df_m36_Broad_Attn.dropna()   # remove NaN\n",
    "# df_m36_Broad_Attn = df_m36_Broad_Attn.astype(int)\n",
    "# # df_m36_Broad_Attn.to_csv((r'./Data/Neurocognitive_Scores/m36_Broad_Attn.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generating Scores for (36_Months - Baseline_Phase)\n",
    "\n",
    "# ## m36-base & Process_GIA_Score\n",
    "# df_m36_join_base_GIA = pd.merge(df_m36_GIA, df_base_GIA, how='inner', on=['ID'])\n",
    "# df_m36_join_base_GIA['GIA_SS'] = df_m36_join_base_GIA.GIA_SS_x - df_m36_join_base_GIA.GIA_SS_y \n",
    "# df_m36_base_GIA = df_m36_join_base_GIA.drop(['GIA_SS_x', 'GIA_SS_y'], axis=1)\n",
    "# # df_m36_base_GIA.to_csv((r'./Data/Neurocognitive_Scores/m36_base_GIA.csv'), index=False)\n",
    "\n",
    "# ## m36-base & Process_Spead_Score\n",
    "# df_base_Proc_Spd = pd.read_csv('./Data/Neurocognitive_Scores/baseline_Proc_Spd.csv')\n",
    "# df_m36_Proc_Spd = pd.read_csv('./Data/Neurocognitive_Scores/m36_Proc_Spd.csv')\n",
    "# df_m36_join_base_Proc_Spd = pd.merge(df_m36_Proc_Spd, df_base_Proc_Spd, how='inner', on=['ID'])\n",
    "# df_m36_join_base_Proc_Spd['Proc_Spd_SS'] = df_m36_join_base_Proc_Spd.Proc_Spd_SS_x - df_m36_join_base_Proc_Spd.Proc_Spd_SS_y \n",
    "# df_m36_base_Proc_Spd = df_m36_join_base_Proc_Spd.drop(['Proc_Spd_SS_x', 'Proc_Spd_SS_y'], axis=1)\n",
    "# # df_m36_base_Proc_Spd.to_csv((r'./Data/Neurocognitive_Scores/m36_base_Proc_Spd.csv'), index=False)\n",
    "\n",
    "# ## m36-base & Working_Memory_Score\n",
    "# df_base_Working_Mem = pd.read_csv('./Data/Neurocognitive_Scores/baseline_Working_Mem.csv')\n",
    "# df_m36_Working_Mem = pd.read_csv('./Data/Neurocognitive_Scores/m36_Working_Mem.csv')\n",
    "# df_m36_join_base_Working_Mem = pd.merge(df_m36_Working_Mem, df_base_Working_Mem, how='inner', on=['ID'])\n",
    "# df_m36_join_base_Working_Mem['Working_Mem_SS'] = df_m36_join_base_Working_Mem.Working_Mem_SS_x - df_m36_join_base_Working_Mem.Working_Mem_SS_y \n",
    "# df_m36_base_Working_Mem = df_m36_join_base_Working_Mem.drop(['Working_Mem_SS_x', 'Working_Mem_SS_y'], axis=1)\n",
    "# # df_m36_base_Working_Mem.to_csv((r'./Data/Neurocognitive_Scores/m36_base_Working_Mem.csv'), index=False)\n",
    "\n",
    "# ## m36-base & Broad_Attention_Score\n",
    "# df_base_Broad_Attn = pd.read_csv('./Data/Neurocognitive_Scores/baseline_Broad_Attn.csv')\n",
    "# df_m36_Broad_Attn = pd.read_csv('./Data/Neurocognitive_Scores/m36_Broad_Attn.csv')\n",
    "# df_m36_join_base_Broad_Attn = pd.merge(df_m36_Broad_Attn, df_base_Broad_Attn, how='inner', on=['ID'])\n",
    "# df_m36_join_base_Broad_Attn['Broad_Attn_SS'] = df_m36_join_base_Broad_Attn.Broad_Attn_SS_x - df_m36_join_base_Broad_Attn.Broad_Attn_SS_y \n",
    "# df_m36_base_Broad_Attn = df_m36_join_base_Broad_Attn.drop(['Broad_Attn_SS_x', 'Broad_Attn_SS_y'], axis=1)\n",
    "# # df_m36_base_Broad_Attn.to_csv((r'./Data/Neurocognitive_Scores/m36_base_Broad_Attn.csv'), index=False)\n",
    "\n",
    "# ## m36-base for All Scores\n",
    "# df_m36base_all_scores = pd.merge(cluster_m36base_GIA, cluster_m36base_Proc_Spd, how='outer', on=['ID', 'class_e500', 'class_e1500'])\n",
    "# df_m36base_all_scores = pd.merge(df_m36base_all_scores, cluster_m36base_Working_Mem, how='outer', on=['ID', 'class_e500', 'class_e1500'])\n",
    "# df_m36base_all_scores = pd.merge(df_m36base_all_scores, cluster_m36base_Broad_Attn, how='outer', on=['ID', 'class_e500', 'class_e1500'])\n",
    "# df_m36base_all_scores = df_m36base_all_scores.drop(['class_e500', 'class_e1500'], axis=1)\n",
    "# # df_m36base_all_scores.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/df_m36base_all_scores.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Master Neurocognitive Scores\n",
    "\n",
    "# df_base = pd.read_csv('../Data/data_random_1/Neurocognitive/baseline.csv')\n",
    "# df_base = df_base.drop(['Phase_Description'], axis=1)\n",
    "# df_base = df_base.rename(columns={'GIA_SS': 'GIA_base', 'Proc_Spd_SS': 'Proc_Spd_base', \n",
    "#                                   'Working_Mem_SS': 'Working_Mem_base', 'Broad_Attn_SS': 'Broad_Attn_base'})\n",
    "# df_m36 = pd.read_csv('../Data/data_random_1/Neurocognitive/m36.csv')\n",
    "# df_m36 = df_m36.drop(['Phase_Description'], axis=1)\n",
    "# df_m36 = df_m36.rename(columns={'GIA_SS': 'GIA_36m', 'Proc_Spd_SS': 'Proc_Spd_36m', \n",
    "#                                 'Working_Mem_SS': 'Working_Mem_36m', 'Broad_Attn_SS': 'Broad_Attn_36m'})\n",
    "# df_m36base_all_scores = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/df_m36base_all_scores.csv')\n",
    "# df_m36base_all_scores = df_m36base_all_scores.rename(columns={'GIA_SS': 'GIA_diff', 'Proc_Spd_SS': 'Proc_Spd_diff', \n",
    "#                                                               'Working_Mem_SS': 'Working_Mem_diff', 'Broad_Attn_SS': 'Broad_Attn_diff'})\n",
    "# # ## Inner Join\n",
    "# # master_score = pd.merge(df_base, df_m36, how='inner', on=['ID'])\n",
    "# # master_score = pd.merge(master_score, df_m36base_all_scores, how='inner', on=['ID'])\n",
    "# # # master_score.to_csv('../Data/data_random_1/Neurocognitive_Joins/master_score.csv', index=False)\n",
    "\n",
    "# ## Outer Join\n",
    "# master_score = pd.merge(df_base, df_m36, how='outer', on=['ID'])\n",
    "# master_score = pd.merge(master_score, df_m36base_all_scores, how='outer', on=['ID'])\n",
    "# # master_score.to_csv('../Data/data_random_1/Neurocognitive_Joins/master_score_union.csv', index=False)\n",
    "\n",
    "# master_score = pd.read_csv(r'../Data/data_random_1/Neurocognitive_Joins/master_score.csv')\n",
    "# master_score = pd.read_csv(r'../Data/data_random_1/Neurocognitive_Joins/master_score_union.csv')\n",
    "# master_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Joining of Cluster with Baseline\n",
    "\n",
    "# ## Cluster--*--GIA for base\n",
    "# df = pd.merge(df_clusters, df_base_GIA, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_base_GIA = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_base_GIA.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_base_GIA.csv'))\n",
    "\n",
    "# ## Cluster--*--ProcSpd for base\n",
    "# df = pd.merge(df_clusters, df_base_ProcSpd, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_base_ProcSpd = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_base_ProcSpd.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_base_Proc_Spd.csv'))\n",
    "\n",
    "# ## Cluster--*--WorkingMem for base\n",
    "# df = pd.merge(df_clusters, df_base_WorkingMem, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_base_WorkingMem = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_base_WorkingMem.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_base_Working_Mem.csv'))\n",
    "\n",
    "# ## Cluster--*--BroadAttn for base\n",
    "# df = pd.merge(df_clusters, df_base_BroadAttn, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_base_BroadAttn = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_base_BroadAttn.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_base_Broad_Attn.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Joining of Cluster with (36month - baseline)\n",
    "\n",
    "# ## Cluster--*--GIA for m36-base\n",
    "# df = pd.merge(df_clusters, df_m36_base_GIA, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_m36base_GIA = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_m36base_GIA.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_GIA.csv'))\n",
    "\n",
    "# ## Cluster--*--ProcSpd for m36-base\n",
    "# df = pd.merge(df_clusters, df_m36_base_ProcSpd, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_m36base_ProcSpd = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_m36base_ProcSpd.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Proc_Spd.csv'))\n",
    "\n",
    "# ## Cluster--*--WorkingMem for m36-base\n",
    "# df = pd.merge(df_clusters, df_m36_base_WorkingMem, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_m36base_WorkingMem = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_m36base_WorkingMem.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Working_Mem.csv'))\n",
    "\n",
    "# ## Cluster--*--BroadAttn for m36-base\n",
    "# df = pd.merge(df_clusters, df_m36_base_BroadAttn, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_m36base_BroadAttn = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_m36base_BroadAttn.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Broad_Attn.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Joining of All scores with LF \n",
    "\n",
    "# ## Joining All Scores for Baseline-Phase\n",
    "# df_base_all_scores = pd.merge(df_clusters, df_base, how='inner', on=['ID'])\n",
    "# df_base_all_scores = df_base_all_scores.drop(['same_clusters', 'Phase_Description'], axis=1)\n",
    "# # df_base_all_scores.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/df_base_all_scores.csv'))\n",
    "\n",
    "# ## Joining All Scores for 36m-base\n",
    "# df_m36_all_scores = pd.merge(df_clusters, df_m36, how='inner', on=['ID'])\n",
    "# df_m36_all_scores = df_m36_all_scores.drop(['same_clusters', 'Phase_Description'], axis=1)\n",
    "# # df_m36_all_scores.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/df_m36_all_scores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cluster_join_base_GIA         = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_base_GIA.csv')\n",
    "# df_cluster_join_base_Proc_Spd    = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_base_Proc_Spd.csv')\n",
    "# df_cluster_join_base_Working_Mem = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_base_Working_Mem.csv')\n",
    "# df_cluster_join_base_Broad_Attn  = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_base_Broad_Attn.csv')\n",
    "\n",
    "# df_cluster_join_m36base_GIA         = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_GIA.csv')\n",
    "# df_cluster_join_m36base_Proc_Spd    = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Proc_Spd.csv')\n",
    "# df_cluster_join_m36base_Working_Mem = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Working_Mem.csv')\n",
    "# df_cluster_join_m36base_Broad_Attn  = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Broad_Attn.csv')\n",
    "\n",
    "# df_base = pd.read_csv('../Data/data_random_1/Neurocognitive/baseline.csv')\n",
    "# df_m36 = pd.read_csv('../Data/data_random_1/Neurocognitive/m36.csv')\n",
    "\n",
    "# df_base_all_scores    = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/df_base_all_scores.csv')\n",
    "# df_m36_all_scores     = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/df_m36_all_scores.csv')\n",
    "# df_m36base_all_scores = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/df_m36base_all_scores.csv')\n",
    "\n",
    "# print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "#       'there are {} common records for GIA Score'.format(len(df_cluster_join_base_GIA)))\n",
    "# print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "#       'there are {} common records for Process Speed Score'.format(len(df_cluster_join_base_Proc_Spd)))\n",
    "# print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "#       'there are {} common records for Working Memory Score'.format(len(df_cluster_join_base_Working_Mem)))\n",
    "# print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "#       'there are {} common records for Broad Attention Score\\n'.format(len(df_cluster_join_base_Broad_Attn)))\n",
    "\n",
    "# print('\\033[1m Among {} records of Month-36,'.format(len(df_m36)) , 'and {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "#       'there are {} common records for GIA Score'.format(len(df_cluster_join_m36base_GIA)))\n",
    "# print('\\033[1m Among {} records of Month-36,'.format(len(df_m36)) , 'and {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "#       'there are {} common records for Process Speed Score'.format(len(df_cluster_join_m36base_Proc_Spd)))\n",
    "# print('\\033[1m Among {} records of Month-36,'.format(len(df_m36)) , 'and {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "#       'there are {} common records for Working Memory Score'.format(len(df_cluster_join_m36base_Working_Mem)))\n",
    "# print('\\033[1m Among {} records of Month-36,'.format(len(df_m36)) , 'and {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "#       'there are {} common records for Broad Attention Score\\n'.format(len(df_cluster_join_m36base_Broad_Attn)))\n",
    "\n",
    "# print('\\033[1m There are totally {} common records for all scores in baseline'.format(len(df_base_all_scores)))\n",
    "# print('\\033[1m There are totally {} common records for all scores in 36months)'.format(len(df_m36_all_scores)))\n",
    "# print('\\033[1m There are totally {} common records for all scores in (36months-baseline)'.format(len(df_m36base_all_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining of LF_500 with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LF_500 = pd.read_csv('../Data/data_random_1/LF/LF124_e500.csv')\n",
    "# # LF_500.index = LF_500[LF_500.columns[0]]\n",
    "# # LF_500 = LF_500.rename(columns={'Patient_ID':'ID'})\n",
    "# LF_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## LF_500 for Base scores\n",
    "# LF_e500_base_GIA = pd.merge(df_cluster_join_base_GIA, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_base_GIA.index = LF_e500_base_GIA.ID\n",
    "# LF_e500_base_GIA = LF_e500_base_GIA.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_base_GIA.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_base_GIA.csv')\n",
    "\n",
    "# LF_e500_base_Proc_Spd = pd.merge(df_cluster_join_base_Proc_Spd, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_base_Proc_Spd.index = LF_e500_base_Proc_Spd.ID\n",
    "# LF_e500_base_Proc_Spd = LF_e500_base_Proc_Spd.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_base_Proc_Spd.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_base_Proc_Spd.csv')\n",
    "\n",
    "# LF_e500_base_Working_Mem = pd.merge(df_cluster_join_base_Working_Mem, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_base_Working_Mem.index = LF_e500_base_Working_Mem.ID\n",
    "# LF_e500_base_Working_Mem = LF_e500_base_Working_Mem.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_base_Working_Mem.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_base_Working_Mem.csv')\n",
    "\n",
    "# LF_e500_base_Broad_Attn = pd.merge(df_cluster_join_base_Broad_Attn, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_base_Broad_Attn.index = LF_e500_base_Broad_Attn.ID\n",
    "# LF_e500_base_Broad_Attn = LF_e500_base_Broad_Attn.drop(['ID', 'class_e1500'], axis=1)\n",
    "# LF_e500_base_Broad_Attn.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_base_Broad_Attn.csv')\n",
    "\n",
    "# ##---------------------------------------------------------------------------------\n",
    "# ## LF_500 for (Month36-Base) scores\n",
    "# LF_e500_m36base_GIA = pd.merge(df_cluster_join_m36base_GIA, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_m36base_GIA.index = LF_e500_m36base_GIA.ID\n",
    "# LF_e500_m36base_GIA = LF_e500_m36base_GIA.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_m36base_GIA.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_m36base_GIA.csv')\n",
    "\n",
    "# LF_e500_m36base_Proc_Spd = pd.merge(df_cluster_join_m36base_Proc_Spd, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_m36base_Proc_Spd.index = LF_e500_m36base_Proc_Spd.ID\n",
    "# LF_e500_m36base_Proc_Spd = LF_e500_m36base_Proc_Spd.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_m36base_Proc_Spd.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_m36base_Proc_Spd.csv')\n",
    "\n",
    "# LF_e500_m36base_Working_Mem = pd.merge(df_cluster_join_m36base_Working_Mem, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_m36base_Working_Mem.index = LF_e500_m36base_Working_Mem.ID\n",
    "# LF_e500_m36base_Working_Mem = LF_e500_m36base_Working_Mem.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_m36base_Working_Mem.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_m36base_Working_Mem.csv')\n",
    "\n",
    "# LF_e500_m36base_Broad_Attn = pd.merge(df_cluster_join_m36base_Broad_Attn, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_m36base_Broad_Attn.index = LF_e500_m36base_Broad_Attn.ID\n",
    "# LF_e500_m36base_Broad_Attn = LF_e500_m36base_Broad_Attn.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_m36base_Broad_Attn.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_m36base_Broad_Attn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## LF_500 for Base scores\n",
    "# LF_e500_base_GIA         = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_base_GIA.csv')\n",
    "# LF_e500_base_Proc_Spd    = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_base_Proc_Spd.csv')\n",
    "# LF_e500_base_Working_Mem = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_base_Working_Mem.csv')\n",
    "# LF_e500_base_Broad_Attn  = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_base_Broad_Attn.csv')\n",
    "\n",
    "# ## LF_500 for (Month36-Base) scores\n",
    "# LF_e500_m36base_GIA         = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_m36base_GIA.csv')\n",
    "# LF_e500_m36base_Proc_Spd    = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_m36base_Proc_Spd.csv')\n",
    "# LF_e500_m36base_Working_Mem = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_m36base_Working_Mem.csv')\n",
    "# LF_e500_m36base_Broad_Attn  = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_m36base_Broad_Attn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining of LF_1500 with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LF_1500 = pd.read_csv('../Data/data_random_1/LF/LF124_e1500.csv')\n",
    "# LF_1500.index = LF_1500[LF_1500.columns[0]]\n",
    "# LF_1500 = LF_1500.rename(columns={'Patient_ID':'ID'})\n",
    "# LF_1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## LF_1500 for Base scores\n",
    "# LF_e1500_base_GIA = pd.merge(df_cluster_join_base_GIA, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_base_GIA.index = LF_e1500_base_GIA.ID\n",
    "# LF_e1500_base_GIA = LF_e1500_base_GIA.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_base_GIA.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_base_GIA.csv')\n",
    "\n",
    "# LF_e1500_base_Proc_Spd = pd.merge(df_cluster_join_base_Proc_Spd, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_base_Proc_Spd.index = LF_e1500_base_Proc_Spd.ID\n",
    "# LF_e1500_base_Proc_Spd = LF_e1500_base_Proc_Spd.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_base_Proc_Spd.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_base_Proc_Spd.csv')\n",
    "\n",
    "# LF_e1500_base_Working_Mem = pd.merge(df_cluster_join_base_Working_Mem, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_base_Working_Mem.index = LF_e1500_base_Working_Mem.ID\n",
    "# LF_e1500_base_Working_Mem = LF_e1500_base_Working_Mem.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_base_Working_Mem.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_base_Working_Mem.csv')\n",
    "\n",
    "# LF_e1500_base_Broad_Attn = pd.merge(df_cluster_join_base_Broad_Attn, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_base_Broad_Attn.index = LF_e1500_base_Broad_Attn.ID\n",
    "# LF_e1500_base_Broad_Attn = LF_e1500_base_Broad_Attn.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_base_Broad_Attn.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_base_Broad_Attn.csv')\n",
    "\n",
    "# # ##---------------------------------------------------------------------------------\n",
    "# # ## LF_1500 for (Month36-Base) scores\n",
    "# LF_e1500_m36base_GIA = pd.merge(df_cluster_join_m36base_GIA, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_m36base_GIA.index = LF_e1500_m36base_GIA.ID\n",
    "# LF_e1500_m36base_GIA = LF_e1500_m36base_GIA.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_m36base_GIA.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_m36base_GIA.csv')\n",
    "\n",
    "# LF_e1500_m36base_Proc_Spd = pd.merge(df_cluster_join_m36base_Proc_Spd, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_m36base_Proc_Spd.index = LF_e1500_m36base_Proc_Spd.ID\n",
    "# LF_e1500_m36base_Proc_Spd = LF_e1500_m36base_Proc_Spd.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_m36base_Proc_Spd.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_m36base_Proc_Spd.csv')\n",
    "\n",
    "# LF_e1500_m36base_Working_Mem = pd.merge(df_cluster_join_m36base_Working_Mem, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_m36base_Working_Mem.index = LF_e1500_m36base_Working_Mem.ID\n",
    "# LF_e1500_m36base_Working_Mem = LF_e1500_m36base_Working_Mem.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_m36base_Working_Mem.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_m36base_Working_Mem.csv')\n",
    "\n",
    "# LF_e1500_m36base_Broad_Attn = pd.merge(df_cluster_join_m36base_Broad_Attn, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_m36base_Broad_Attn.index = LF_e1500_m36base_Broad_Attn.ID\n",
    "# LF_e1500_m36base_Broad_Attn = LF_e1500_m36base_Broad_Attn.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_m36base_Broad_Attn.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_m36base_Broad_Attn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
