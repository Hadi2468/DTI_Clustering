{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some helpful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Version control\n",
      "------------------------\n",
      "Numpy\t\t 1.19.4\n",
      "matplotlib\t 3.3.3\n",
      "NiBabel\t\t 3.2.0\n",
      "Pandas\t\t 1.1.4\n",
      "imageio\t\t 2.9.0\n",
      "H5py\t\t 2.10.0\n",
      "Scikit-learn\t 0.23.2\n",
      "Scikit-image\t 0.17.2\n",
      "TensorFlow\t 2.4.0\n",
      "Keras\t\t 2.4.3\n"
     ]
    }
   ],
   "source": [
    "print(\"    Version control\\n------------------------\")\n",
    "import os, fnmatch, random, math, sys, datetime\n",
    "from pathlib import Path\n",
    "import numpy as np;              print(\"Numpy\\t\\t\", np.__version__)\n",
    "import matplotlib as mpl;        print(\"matplotlib\\t\", mpl.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib;           print(\"NiBabel\\t\\t {}\".format(nib.__version__))\n",
    "from nibabel.testing import data_path\n",
    "import pandas as pd;             print(\"Pandas\\t\\t {}\".format(pd.__version__))\n",
    "import imageio;                  print(\"imageio\\t\\t {}\".format(imageio.__version__))\n",
    "import h5py;                     print(\"H5py\\t\\t {}\".format(h5py.__version__))\n",
    "import sklearn;                  print(\"Scikit-learn\\t {}\".format(sklearn.__version__))\n",
    "import skimage;                  print(\"Scikit-image\\t {}\".format(skimage.__version__))\n",
    "import tensorflow as tf;         print(\"TensorFlow\\t {}\".format(tf.__version__))\n",
    "import keras as K;               print(\"Keras\\t\\t {}\".format(K.__version__))\n",
    "from tensorflow.keras import models, Input, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, BatchNormalization, Conv3D, MaxPooling3D, UpSampling3D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "# %load_ext tensorboard       # %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (100, 182, 218, 182, 1)\n",
      "valid_data shape is (24, 182, 218, 182, 1)\n"
     ]
    }
   ],
   "source": [
    "## Load train data\n",
    "data_source = \"Data/data_random_1/\" \n",
    "sample_train_subset = np.loadtxt(os.path.join(data_source, \"sample_train.csv\"), dtype=str, delimiter=\",\")\n",
    "train_data = np.load(os.path.join(data_source, \"train.npy\")).reshape(100,182,218,182,1)\n",
    "print('train_data shape is {}'.format(train_data.shape))\n",
    "\n",
    "## Load validation data\n",
    "sample_val_subset = np.loadtxt(os.path.join(data_source, \"sample_valid.csv\"), dtype=str, delimiter=\",\")\n",
    "valid_data = np.load(os.path.join(data_source, \"valid.npy\")).reshape(24,182,218,182,1)\n",
    "print('valid_data shape is {}'.format(valid_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"There are\", len(sample_train_subset), \" subset of train samples are:\\n\")\n",
    "# print(*sample_train_subset, sep='\\t')\n",
    "# print(\"\\n--------------------------------------------------------------------------------\\n\")\n",
    "# print(\"There are\", len(sample_val_subset), \" subset of Validation samples are:\\n\")\n",
    "# print(*sample_val_subset, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Showing one or all Training samples in three dimension (one middle slice per each dimension)\n",
    "\n",
    "# def show_slices(slices):\n",
    "#     fig, axes = plt.subplots(1, len(slices), figsize=(10,5))\n",
    "#     for i, slice in enumerate(slices):\n",
    "#         axes[i].imshow(slice.T, cmap=\"hot\", origin=\"upper\") # hot, Greys, gray\n",
    "        \n",
    "# # for m in range(train_data.shape[0]):\n",
    "# for m in range(1):\n",
    "#     slice_0 = train_data[m, 91, :, :, 0]\n",
    "#     slice_1 = train_data[m, :, 109, :, 0]\n",
    "#     slice_2 = train_data[m, :, :, 91, 0]\n",
    "#     show_slices([slice_0, slice_1, slice_2])\n",
    "#     plt.suptitle(sample_train_subset[m], x=0.5, y=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Showing one or all Validation samples in three dimension (one middle slice per each dimension)\n",
    "\n",
    "# # for m in range(valid_data.shape[0]):\n",
    "# for m in range(1):\n",
    "#     slice_0 = valid_data[m, 91, :, :, 0]\n",
    "#     slice_1 = valid_data[m, :, 109, :, 0]\n",
    "#     slice_2 = valid_data[m, :, :, 91, 0]\n",
    "#     show_slices([slice_0, slice_1, slice_2])\n",
    "#     plt.suptitle(sample_val_subset[m], x=0.5, y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AutoEncoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 182, 218, 182, 1) 0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv3D)               (None, 91, 109, 91, 1)    28        \n",
      "_________________________________________________________________\n",
      "BN_Conv1 (BatchNormalization (None, 91, 109, 91, 1)    4         \n",
      "_________________________________________________________________\n",
      "Flat (Flatten)               (None, 902629)            0         \n",
      "_________________________________________________________________\n",
      "UnFlat (Reshape)             (None, 91, 109, 91, 1)    0         \n",
      "_________________________________________________________________\n",
      "DeConv6 (Conv3D)             (None, 91, 109, 91, 1)    2         \n",
      "_________________________________________________________________\n",
      "BN_DeConv1 (BatchNormalizati (None, 91, 109, 91, 1)    4         \n",
      "_________________________________________________________________\n",
      "UpSampling1 (UpSampling3D)   (None, 182, 218, 182, 1)  0         \n",
      "_________________________________________________________________\n",
      "Output (Conv3D)              (None, 182, 218, 182, 1)  28        \n",
      "=================================================================\n",
      "Total params: 66\n",
      "Trainable params: 62\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Test simple model:\n",
    "\n",
    "IMAGE_HEIGHT = train_data.shape[1]\n",
    "IMAGE_WIDTH = train_data.shape[2]\n",
    "IMAGE_DEPTH = train_data.shape[3]\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "data_shape = [1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "input_shape = [BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "## Encoder\n",
    "input_img = Input(shape=(182, 218, 182, 1), name='Input')\n",
    "x = Conv3D(filters=1, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv1')(input_img)\n",
    "x = BatchNormalization(name='BN_Conv1')(x)\n",
    "## Latent Features\n",
    "shape_before_flattening = tf.keras.backend.int_shape(x)\n",
    "x = Flatten(name='Flat')(x)\n",
    "encoded = x\n",
    "x = Reshape(shape_before_flattening[1:], name='UnFlat')(x)\n",
    "## Decoder\n",
    "x = Conv3D(filters=1, kernel_size=1, padding='valid', activation='relu', name='DeConv6')(x)\n",
    "x = BatchNormalization(name='BN_DeConv1')(x)\n",
    "x = UpSampling3D(size=(2, 2, 2), name='UpSampling1')(x)\n",
    "decoded = Conv3D(filters=1, kernel_size=3, padding='same', activation='sigmoid', name='Output')(x)\n",
    "model_CAE = Model(inputs=input_img, outputs=decoded, name='AutoEncoder')\n",
    "model_CAE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model_CAE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Define parameters:\n",
    "\n",
    "# IMAGE_HEIGHT = train_data.shape[1]\n",
    "# IMAGE_WIDTH = train_data.shape[2]\n",
    "# IMAGE_DEPTH = train_data.shape[3]\n",
    "# BATCH_SIZE = 1\n",
    "# EPOCHS = 1\n",
    "# data_shape = [1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "# input_shape = [BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "# print(\"input-layer shape:\", input_shape)\n",
    "\n",
    "# ## Encoder\n",
    "# input_img = Input(shape=(182, 218, 182, 1), name='Input')\n",
    "# x = Conv3D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv1')(input_img)\n",
    "# x = BatchNormalization(name='BN_Conv1')(x)\n",
    "# x = Conv3D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv2')(x)\n",
    "# x = BatchNormalization(name='BN_Conv2')(x)\n",
    "# x = Conv3D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv3')(x)\n",
    "# x = BatchNormalization(name='BN_Conv3')(x)\n",
    "# x = Conv3D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv4')(x)\n",
    "# x = BatchNormalization(name='BN_Conv4')(x)\n",
    "# x = Conv3D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv5')(x)\n",
    "# x = BatchNormalization(name='BN_Conv5')(x)\n",
    "# x = Conv3D(filters=4, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv6')(x)\n",
    "# x = BatchNormalization(name='BN_Conv6')(x)\n",
    "\n",
    "# ## Latent Features\n",
    "# shape_before_flattening = tf.keras.backend.int_shape(x)\n",
    "# x = Flatten(name='LF')(x)\n",
    "# # init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n",
    "# # encoded = Dense(50, kernel_initializer=init, activation='relu', name='encoded')(x)\n",
    "# # encoded = Dense(50, activation='relu', name='encoded')(x)\n",
    "# encoded = x\n",
    "# # x = BatchNormalization()(encoded)\n",
    "# # x = Dense(np.prod(shape_before_flattening[1:]), activation='relu', kernel_initializer=init)(encoded)\n",
    "# # x = Dense(np.prod(shape_before_flattening[1:]), activation='relu')(encoded)\n",
    "# x = Reshape(shape_before_flattening[1:], name='UnFlat')(x)\n",
    "\n",
    "# ## Decoder\n",
    "# x = Conv3D(filters=4, kernel_size=3, padding='same', activation='relu', name='DeConv1')(x)\n",
    "# x = BatchNormalization(name='BN_DeConv1')(x)\n",
    "# x = UpSampling3D(size=(2, 2, 2), name='UpSampling1')(x)\n",
    "# x = Conv3D(filters=8, kernel_size=(1,2,1), padding='valid', activation='relu', name='DeConv2')(x)\n",
    "# x = BatchNormalization(name='BN_DeConv2')(x)\n",
    "# x = UpSampling3D(size=(2, 2, 2), name='UpSampling2')(x)\n",
    "# x = Conv3D(filters=16, kernel_size=3, padding='same', activation='relu', name='DeConv3')(x)\n",
    "# x = BatchNormalization(name='BN_DeConv3')(x)\n",
    "# x = UpSampling3D(size=(2, 2, 2), name='UpSampling3')(x)\n",
    "# x = Conv3D(filters=32, kernel_size=(2,1,2), padding='valid', activation='relu', name='DeConv4')(x)\n",
    "# x = BatchNormalization(name='BN_DeConv4')(x)\n",
    "# x = UpSampling3D(size=(2, 2, 2), name='UpSampling4')(x)\n",
    "# x = Conv3D(filters=64, kernel_size=(1,2,1), padding='valid', activation='relu', name='DeConv5')(x)\n",
    "# x = BatchNormalization(name='BN_DeConv5')(x)\n",
    "# x = UpSampling3D(size=(2, 2, 2), name='UpSampling5')(x)\n",
    "# x = Conv3D(filters=128, kernel_size=2, padding='valid', activation='relu', name='DeConv6')(x)\n",
    "# x = BatchNormalization(name='BN_DeConv6')(x)\n",
    "# x = UpSampling3D(size=(2, 2, 2), name='UpSampling6')(x)\n",
    "# decoded = Conv3D(filters=1, kernel_size=3, padding='same', activation='sigmoid', name='Output')(x)\n",
    "\n",
    "# model_CAE = Model(inputs=input_img, outputs=decoded, name='AutoEncoder')\n",
    "# ## optimizer=rmsprop, sgd\n",
    "# model_CAE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "# model_CAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Time = 2021_01_02___16_29 \n",
      "\n",
      "10/10 [==============================] - 78s 8s/step - loss: 0.0268 - accuracy: 0.6888 - val_loss: 0.0155 - val_accuracy: 0.7067\n",
      "\n",
      "Epoch 00001: val_loss improved from -inf to 0.01552, saving model to Check/L100___2021_01_02___16_29\n",
      "\n",
      "End Time = 2021_01_02____16_30\n"
     ]
    }
   ],
   "source": [
    "## Start time:\n",
    "from datetime import datetime\n",
    "start_time = datetime.now().strftime(\"%Y_%m_%d___%H_%M\"); print(\"\\nStart Time =\", start_time, \"\\n\")\n",
    "\n",
    "## Model Fit\n",
    "model_CAE.load_weights(os.path.join(\"Weights/L100___2021_01_01___23_46.hdf5\"), by_name=True)\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=os.path.join(\"Check/L100___\" + start_time), save_weights_only=True, save_best_only=True, monitor='val_loss', mode='max', verbose=1) \n",
    "tb_callback = TensorBoard(os.path.join(\"Logs/L100___\" + start_time), histogram_freq=1)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\n",
    "model_CAE.fit(train_data[0:10,:], train_data[0:10,:], validation_data=(valid_data, valid_data), epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=[tb_callback, model_checkpoint_callback], verbose=1)\n",
    "model_CAE.save_weights(os.path.join(\"Weights/L100___\" + start_time + \".hdf5\"))\n",
    "\n",
    "## End time:\n",
    "# from datetime import datetime\n",
    "end_time = datetime.now().strftime(\"%Y_%m_%d____%H_%M\"); print(\"\\nEnd Time =\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = valid_data[0,:].reshape(1, 182, 218, 182, 1)\n",
    "reconstructed = model_CAE.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in range(1):\n",
    "#     slice_0 = reconstructed[m, 91, :, :, 0]\n",
    "#     slice_1 = reconstructed[m, :, 109, :, 0]\n",
    "#     slice_2 = reconstructed[m, :, :, 91, 0]\n",
    "#     show_slices([slice_0, slice_1, slice_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\ntrain_data[0,100,100:105,100]\\n\\n {}'.format(train_data[0,100,100:105,100]),'\\n')\n",
    "# print('\\nReconstructed_data[0,100,100:105,100]\\n\\n {}'.format(reconstructed[0,100,100:105,100]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5_file = h5py.File(os.path.join(\"Weights/L100___\" + start_time + \".hdf5\"), 'r')\n",
    "# # h5_file = h5py.File(os.path.join(\"Weights/w_Pegasus_128.hdf5\"), 'r')\n",
    "# Layer_size = len(list(h5_file.keys()))\n",
    "# Layer_names = list(h5_file.keys())\n",
    "# print(\"There are\", Layer_size, \"layers in this model as:\\n\\n\", Layer_names,'\\n')\n",
    "\n",
    "# for l in range(5):  #Layer_size\n",
    "#     print('==========================================================\\n')\n",
    "#     layers = h5_file[Layer_names[l]]\n",
    "# #     print(\"Layer\", l+1, \"-----\", layers)\n",
    "#     W = layers[Layer_names[l]]['kernel:0']\n",
    "#     print('Layer', l+1, ':', list(h5_file.keys())[l], '\\tWeights\\' shape: {}'.format(W.shape), '\\n')\n",
    "# #     print('\\nWeights[1][1][1]: {}'.format(W[1][1][1]))\n",
    "    \n",
    "#     Kernel_1 = W.shape[0]\n",
    "#     Kernel_2 = W.shape[1]\n",
    "#     Kernel_3 = W.shape[2]\n",
    "#     Kernel_all = np.zeros([Kernel_1, Kernel_2, Kernel_3])\n",
    "#     for f in range(2):   # W.shape[4]\n",
    "#         for x in range(Kernel_1):\n",
    "#             for y in range(Kernel_2):\n",
    "#                 for z in range(Kernel_3):\n",
    "#                     Kernel_all[x][y][z] = (W[x][y][z])[0][0]\n",
    "#         print('\\nWeights of kernel', f+1, 'of', W.shape[4], ':\\n\\n', Kernel_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=Logs       # http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer, InputSpec\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCEC(object):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 filters=[32, 64, 128, 10],\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0):\n",
    "\n",
    "        super(DCEC, self).__init__()\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_shape = input_shape\n",
    "        self.alpha = alpha\n",
    "        self.pretrained = False\n",
    "        self.y_pred = []\n",
    "\n",
    "        self.cae = CAE(input_shape, filters)\n",
    "        hidden = self.cae.get_layer(name='embedding').output\n",
    "        self.encoder = Model(inputs=self.cae.input, outputs=hidden)\n",
    "\n",
    "        # Define DCEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(hidden)\n",
    "        self.model = Model(inputs=self.cae.input,\n",
    "                           outputs=[clustering_layer, self.cae.output])\n",
    "\n",
    "    def pretrain(self, x, batch_size=256, epochs=200, optimizer='adam', save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.cae.compile(optimizer=optimizer, loss='mse')\n",
    "        from keras.callbacks import CSVLogger\n",
    "        csv_logger = CSVLogger(args.save_dir + '/pretrain_log.csv')\n",
    "\n",
    "        # begin training\n",
    "        t0 = time()\n",
    "        self.cae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=[csv_logger])\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        self.cae.save(save_dir + '/pretrain_cae_model.h5')\n",
    "        print('Pretrained weights are saved to %s/pretrain_cae_model.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def extract_feature(self, x):  # extract features from before clustering layer\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        q, _ = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, loss=['kld', 'mse'], loss_weights=[1, 1], optimizer='adam'):\n",
    "        self.model.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer)\n",
    "\n",
    "    def fit(self, x, y=None, batch_size=256, maxiter=2e4, tol=1e-3,\n",
    "            update_interval=140, cae_weights=None, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: pretrain if necessary\n",
    "        t0 = time()\n",
    "        if not self.pretrained and cae_weights is None:\n",
    "            print('...pretraining CAE using default hyper-parameters:')\n",
    "            print('   optimizer=\\'adam\\';   epochs=200')\n",
    "            self.pretrain(x, batch_size, save_dir=save_dir)\n",
    "            self.pretrained = True\n",
    "        elif cae_weights is not None:\n",
    "            self.cae.load_weights(cae_weights)\n",
    "            print('cae_weights is loaded successfully.')\n",
    "\n",
    "        # Step 2: initialize cluster centers using k-means\n",
    "        t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        self.y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(self.y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 3: deep clustering\n",
    "        # logging file\n",
    "        import csv, os\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        logfile = open(save_dir + '/dcec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L', 'Lc', 'Lr'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        t2 = time()\n",
    "        loss = [0, 0, 0]\n",
    "        index = 0\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q, _ = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                self.y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(metrics.acc(y, self.y_pred), 5)\n",
    "                    nmi = np.round(metrics.nmi(y, self.y_pred), 5)\n",
    "                    ari = np.round(metrics.ari(y, self.y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=loss[0], Lc=loss[1], Lr=loss[2])\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                y_pred_last = np.copy(self.y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            if (index + 1) * batch_size > x.shape[0]:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size::],\n",
    "                                                 y=[p[index * batch_size::], x[index * batch_size::]])\n",
    "                index = 0\n",
    "            else:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size:(index + 1) * batch_size],\n",
    "                                                 y=[p[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    x[index * batch_size:(index + 1) * batch_size]])\n",
    "                index += 1\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                # save DCEC model checkpoints\n",
    "                print('saving model to:', save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/dcec_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/dcec_model_final.h5')\n",
    "        t3 = time()\n",
    "        print('Pretrain time:  ', t1 - t0)\n",
    "        print('Clustering time:', t3 - t1)\n",
    "        print('Total time:     ', t3 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 182, 218, 182, 1) 0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv3D)               (None, 91, 109, 91, 1)    28        \n",
      "_________________________________________________________________\n",
      "BN_Conv1 (BatchNormalization (None, 91, 109, 91, 1)    4         \n",
      "_________________________________________________________________\n",
      "Flat (Flatten)               (None, 902629)            0         \n",
      "=================================================================\n",
      "Total params: 32\n",
      "Trainable params: 30\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CA = Model(inputs=input_img, outputs=encoded, name='Encoder')\n",
    "model_CA.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model_CA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(model_CA.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_model = Model(inputs=encoder.input, outputs=clustering_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda update -c conda-forge tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed\n",
      "Solving environment: \\ \n",
      "Found conflicts! Looking for incompatible packages.\n",
      "This can take several minutes.  Press CTRL-C to abort.\n",
      "Examining conflict for anaconda glueviz: : 55it [6:12:56, 421.91s/it]                                                                                                                                                  | "
     ]
    }
   ],
   "source": [
    "!conda update -c conda-forge keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
