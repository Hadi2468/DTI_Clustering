{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Version control\n",
      "------------------------\n",
      "Numpy\t\t 1.19.4\n",
      "matplotlib\t 3.3.3\n",
      "NiBabel\t\t 3.2.0\n",
      "Pandas\t\t 1.1.4\n",
      "imageio\t\t 2.9.0\n",
      "H5py\t\t 2.10.0\n",
      "Scikit-learn\t 0.23.2\n",
      "Scikit-image\t 0.17.2\n",
      "TensorFlow\t 2.3.1\n",
      "Keras\t\t 2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "print(\"    Version control\\n------------------------\")\n",
    "import os     # operating system interfaces\n",
    "import fnmatch\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np;              print(\"Numpy\\t\\t\", np.__version__)\n",
    "import matplotlib as mpl;        print(\"matplotlib\\t\", mpl.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib;           print(\"NiBabel\\t\\t {}\".format(nib.__version__))\n",
    "from nibabel.testing import data_path\n",
    "import math\n",
    "import pandas as pd;             print(\"Pandas\\t\\t {}\".format(pd.__version__))\n",
    "import sys\n",
    "import imageio;                  print(\"imageio\\t\\t {}\".format(imageio.__version__))\n",
    "import h5py;                     print(\"H5py\\t\\t {}\".format(h5py.__version__))\n",
    "import sklearn;                  print(\"Scikit-learn\\t {}\".format(sklearn.__version__))\n",
    "import skimage;                  print(\"Scikit-image\\t {}\".format(skimage.__version__))\n",
    "import tensorflow as tf;         print(\"TensorFlow\\t {}\".format(tf.dr__version__))\n",
    "import keras as K;               print(\"Keras\\t\\t {}\".format(K.__version__))\n",
    "from tensorflow.keras import models, Input, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, BatchNormalization, Conv3D, MaxPooling3D, UpSampling3D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shossein/GitHub/DTI_Clustering\n",
      "train_data shape is (100, 182, 218, 182, 1)\n",
      "valid_data shape is (24, 182, 218, 182, 1)\n"
     ]
    }
   ],
   "source": [
    "## Loading dataset from path directory\n",
    "print(os.getcwd())     # '/Users/shossein/GitHub/DTI_Clustering'\n",
    "\n",
    "## load train data\n",
    "sample_train_subset = np.loadtxt(\"train_100_sample_name.csv\", dtype=str, delimiter=\",\")\n",
    "train_data = np.load('train_100.npy').reshape(100,182,218,182,1)\n",
    "print('train_data shape is {}'.format(train_data.shape))\n",
    "\n",
    "## load validation data\n",
    "sample_val_subset = np.loadtxt(\"valid_24_sample_name.csv\", dtype=str, delimiter=\",\")\n",
    "valid_data = np.load('valid_24.npy').reshape(24,182,218,182,1)\n",
    "print('valid_data shape is {}'.format(valid_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input-layer shape: [1, 182, 218, 182, 1]\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 182, 218, 182, 1) 0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv3D)               (None, 91, 109, 91, 1)    28        \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 91, 109, 91, 1)    4         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv3D)               (None, 46, 55, 46, 1)     28        \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 46, 55, 46, 1)     4         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv3D)               (None, 23, 28, 23, 1)     28        \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 23, 28, 23, 1)     4         \n",
      "_________________________________________________________________\n",
      "DeConv1 (Conv3D)             (None, 23, 28, 23, 1)     28        \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 23, 28, 23, 1)     4         \n",
      "_________________________________________________________________\n",
      "up_sampling3d_6 (UpSampling3 (None, 46, 56, 46, 1)     0         \n",
      "_________________________________________________________________\n",
      "DeConv2 (Conv3D)             (None, 46, 56, 46, 1)     28        \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 46, 56, 46, 1)     4         \n",
      "_________________________________________________________________\n",
      "up_sampling3d_7 (UpSampling3 (None, 92, 112, 92, 1)    0         \n",
      "_________________________________________________________________\n",
      "DeConv3 (Conv3D)             (None, 91, 109, 91, 1)    17        \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 91, 109, 91, 1)    4         \n",
      "_________________________________________________________________\n",
      "up_sampling3d_8 (UpSampling3 (None, 182, 218, 182, 1)  0         \n",
      "_________________________________________________________________\n",
      "Output (Conv3D)              (None, 182, 218, 182, 1)  28        \n",
      "=================================================================\n",
      "Total params: 209\n",
      "Trainable params: 197\n",
      "Non-trainable params: 12\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Define parameters:\n",
    "IMAGE_HEIGHT = train_data.shape[1]\n",
    "IMAGE_WIDTH = train_data.shape[2]\n",
    "IMAGE_DEPTH = train_data.shape[3]\n",
    "batch_size = 1\n",
    "data_shape = [1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "input_shape = [batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "print(\"input-layer shape:\", input_shape)\n",
    "\n",
    "## Encoder\n",
    "input_img = Input(shape=(182, 218, 182, 1))\n",
    "x = Conv3D(filters=1, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv1')(input_img)\n",
    "x = BatchNormalization()(x)\n",
    "# x = MaxPooling3D(pool_size=(2, 2, 2), padding='valid')(x)\n",
    "\n",
    "x = Conv3D(filters=1, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "# x = MaxPooling3D(pool_size=(2, 2, 2), padding='valid')(x)\n",
    "\n",
    "x = Conv3D(filters=1, kernel_size=3, strides=2, padding='same', activation='relu', name='Conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "# x = MaxPooling3D(pool_size=(2, 2, 2), padding='valid')(x)\n",
    "\n",
    "## Decoder\n",
    "x = Conv3D(filters=1, kernel_size=3, padding='same', activation='relu', name='DeConv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = UpSampling3D(size=(2, 2, 2))(x)\n",
    "\n",
    "x = Conv3D(filters=1, kernel_size=3, padding='same', activation='relu', name='DeConv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = UpSampling3D(size=(2, 2, 2))(x)\n",
    "\n",
    "x = Conv3D(filters=1, kernel_size=(2,4,2), padding='valid', activation='relu', name='DeConv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = UpSampling3D(size=(2, 2, 2))(x)\n",
    "\n",
    "decoded = Conv3D(filters=1, kernel_size=3, padding='same', activation='sigmoid', name='Output')(x)\n",
    "\n",
    "model_CAE = Model(inputs=input_img, outputs=decoded)\n",
    "## optimizer=rmsprop, sgd    loss=binary_crossentropy, SparseCategoricalCrossentropy(from_logits=False)\n",
    "model_CAE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model_CAE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorboard\n",
    "logdir = os.path.join(\"CAE_logs\", datetime.datetime.now().strftime(\"%Y_%m_%d____%H_%M_%S\"))\n",
    "tb_callback = TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Time = 22:51:01 \n",
      "\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 526s 5s/step - loss: 0.1413 - accuracy: 0.6499 - val_loss: 0.0516 - val_accuracy: 0.7056\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 488s 5s/step - loss: 0.0562 - accuracy: 0.7005 - val_loss: 0.0168 - val_accuracy: 0.7067\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 444s 4s/step - loss: 0.0308 - accuracy: 0.7018 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 476s 5s/step - loss: 0.0183 - accuracy: 0.7023 - val_loss: 0.0151 - val_accuracy: 0.7067\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 472s 5s/step - loss: 0.0155 - accuracy: 0.7023 - val_loss: 0.0151 - val_accuracy: 0.7067\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 420s 4s/step - loss: 0.0151 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 417s 4s/step - loss: 0.0150 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 419s 4s/step - loss: 0.0150 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 424s 4s/step - loss: 0.0150 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 495s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 501s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 486s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 482s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 487s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 490s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 501s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 494s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 485s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 460s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0150 - val_accuracy: 0.7067\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 422s 4s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 423s 4s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 481s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 529s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 523s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 506s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 493s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 519s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 522s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 525s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 485s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 482s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 525s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 500s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 474s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 491s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 482s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 520s 5s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 561s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 565s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 558s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 567s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 563s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 560s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 554s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 560s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 566s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 559s 6s/step - loss: 0.0149 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 555s 6s/step - loss: 0.0148 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 552s 6s/step - loss: 0.0148 - accuracy: 0.7023 - val_loss: 0.0149 - val_accuracy: 0.7067\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 553s 6s/step - loss: 0.0148 - accuracy: 0.7023 - val_loss: 0.0148 - val_accuracy: 0.7067\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to truncate a file which is already open)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-98fb9b079c26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_CAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtb_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_CAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAE_weights.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# autoencoder.load_weights(\"CAE_weights.hdf5\")     # loading weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/m36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[1;32m   2082\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/m36/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/m36/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to truncate a file which is already open)"
     ]
    }
   ],
   "source": [
    "## Start time:\n",
    "from datetime import datetime\n",
    "start_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\nStart Time =\", start_time, \"\\n\")\n",
    "\n",
    "## Midel Fit\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\n",
    "model_CAE.fit(train_data, train_data, epochs=50, batch_size=batch_size, shuffle=True, validation_data=(valid_data, valid_data), callbacks=[tb_callback], verbose=1)\n",
    "model_CAE.save_weights(\"CAE_weights.hdf5\")\n",
    "# autoencoder.load_weights(\"CAE_weights.hdf5\")     # loading weights\n",
    "\n",
    "# End time:\n",
    "from datetime import datetime\n",
    "end_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\nEnd Time =\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CAE.load_weights(\"CAE_weights.hdf5\")     # loading weights\n",
    "test_data = train_data[0,:].reshape(1, 182, 218, 182, 1)\n",
    "reconstructed = model_CAE.predict(test_data)\n",
    "\n",
    "print('\\ntrain_data[0,100,100:110,100]\\n\\n {}'.format(train_data[0,100,100:110,100]),'\\n')\n",
    "print('\\nReconstructed_data[0,100,100:110,100]\\n\\n {}'.format(reconstructed[0,100,100:110,100]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conv1', 'Conv2', 'Conv3', 'DeConv1', 'DeConv2', 'DeConv3', 'Output', 'batch_normalization', 'batch_normalization_1', 'batch_normalization_2', 'batch_normalization_3', 'batch_normalization_4', 'batch_normalization_5', 'input_1', 'up_sampling3d', 'up_sampling3d_1', 'up_sampling3d_2']\n",
      "\n",
      "Weights shape: (3, 3, 3, 1, 64)\n",
      "\n",
      "Weights[1][1][1]: [[ 0.0037242   0.02097484  0.01185212  0.04621937  0.0556117  -0.02928501\n",
      "  -0.02970716 -0.01357271  0.02839159  0.03565723 -0.02163431  0.03573501\n",
      "   0.04866397  0.0811149   0.03360678  0.02316148  0.01399892 -0.04551153\n",
      "  -0.01990886  0.02220522  0.06904128 -0.03217261  0.02329263  0.01268102\n",
      "  -0.03980312  0.03758161 -0.03650775 -0.03783716 -0.03376658 -0.02598745\n",
      "  -0.05506312  0.03708598 -0.02442701  0.02998579  0.01478631 -0.05314519\n",
      "  -0.02072836  0.04768707 -0.03685523 -0.0557698   0.04025914 -0.06789807\n",
      "  -0.03865194  0.00365022  0.0402042   0.05837304  0.07352968  0.02735536\n",
      "   0.00031396  0.03972595  0.00384991 -0.05485379 -0.0627759  -0.03717451\n",
      "  -0.00820512 -0.00876431 -0.01535704  0.00974258 -0.03341849 -0.02033185\n",
      "   0.06515925 -0.05043278 -0.00863289  0.01965425]]\n"
     ]
    }
   ],
   "source": [
    "h5_file = h5py.File('CAE_weights.hdf5', 'r')\n",
    "print(list(h5_file.keys()))\n",
    "# for L in (list(h5_file.keys())):\n",
    "#     print(L)\n",
    "#     L = h5_file[layer]\n",
    "#     W = L[layer]['kernel:0']\n",
    "#     print(W.shape)\n",
    "\n",
    "L = h5_file[list(h5_file.keys())[0]]\n",
    "W = L[list(h5_file.keys())[0]]['kernel:0']\n",
    "print('\\nWeights shape: {}'.format(W.shape))\n",
    "print('\\nWeights[1][1][1]: {}'.format(W[1][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
