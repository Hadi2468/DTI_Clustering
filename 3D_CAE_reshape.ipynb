{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some helpful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version control:\n",
      "\n",
      "Numpy\t\t 1.18.5\n",
      "matplotlib\t 3.3.2\n",
      "NiBabel\t\t 3.2.0\n",
      "Pandas\t\t 1.1.4\n",
      "imageio\t\t 2.9.0\n",
      "H5py\t\t 2.10.0\n",
      "Scikit-learn\t 0.23.2\n",
      "Scikit-image\t 0.17.2\n",
      "TensorFlow\t 2.3.1\n",
      "Keras\t\t 2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "print(\"Version control:\\n\")\n",
    "import os     # operating system interfaces\n",
    "import fnmatch\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np; print(\"Numpy\\t\\t\", np.__version__)\n",
    "import matplotlib as mpl; print(\"matplotlib\\t\", mpl.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib; print(\"NiBabel\\t\\t\", nib.__version__)\n",
    "from nibabel.testing import data_path\n",
    "import math\n",
    "import pandas as pd; print(\"Pandas\\t\\t\", pd.__version__)\n",
    "import sys\n",
    "import imageio; print(\"imageio\\t\\t\", imageio.__version__)\n",
    "import h5py; print(\"H5py\\t\\t\", h5py.__version__)\n",
    "import sklearn; print(\"Scikit-learn\\t\", sklearn.__version__)\n",
    "import skimage; print(\"Scikit-image\\t\", skimage.__version__)\n",
    "import tensorflow as tf; print(\"TensorFlow\\t\", tf.__version__)\n",
    "import keras; print(\"Keras\\t\\t\", keras.__version__)\n",
    "from tensorflow.keras import models, Input, Model\n",
    "from tensorflow.keras.layers import Dense, Conv3D, Conv3DTranspose, MaxPooling3D, UpSampling3D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset:Validation and Training Dataset (randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shossein/GitHub/DTI_Clustering\n",
      "\n",
      "Directory is changed into:\n",
      " /Users/shossein/anaconda3/envs/m36/lib/python3.6/site-packages/nibabel/tests/data/DTI \n",
      "\n",
      "There are 124 samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Loading all datasets from path directory\n",
    "\n",
    "# example_filename = os.path.join(data_path, 'example4d.nii.gz')\n",
    "print(os.getcwd())     # '/Users/shossein/GitHub/DTI_Clustering'\n",
    "dirpath = os.chdir(\"/Users/shossein/anaconda3/envs/m36/lib/python3.6/site-packages/nibabel/tests/data/DTI\")\n",
    "print(\"\\nDirectory is changed into:\\n\", os.getcwd(), \"\\n\") \n",
    "sample_all = []\n",
    "for sample in fnmatch.filter(os.listdir(dirpath), 'Pat_*_1-post.nii.gz'):\n",
    "    sample_all.append(sample)\n",
    "sample_num = len(sample_all)\n",
    "print(\"There are\", sample_num, \"samples.\\n\")\n",
    "sample_all.sort()\n",
    "# print(*sample_all, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Validation samples are:\n",
      "\n",
      "Pat_109_1-post.nii.gz\tPat_114_1-post.nii.gz\tPat_118_1-post.nii.gz\tPat_11_1-post.nii.gz\tPat_129_1-post.nii.gz\tPat_130_1-post.nii.gz\tPat_131_1-post.nii.gz\tPat_137_1-post.nii.gz\tPat_138_1-post.nii.gz\tPat_139_1-post.nii.gz\tPat_144_1-post.nii.gz\tPat_145_1-post.nii.gz\tPat_22_1-post.nii.gz\tPat_24_1-post.nii.gz\tPat_25_1-post.nii.gz\tPat_26_1-post.nii.gz\tPat_60_1-post.nii.gz\tPat_65_1-post.nii.gz\tPat_66_1-post.nii.gz\tPat_76_1-post.nii.gz\tPat_83_1-post.nii.gz\tPat_96_1-post.nii.gz\tPat_97_1-post.nii.gz\tPat_99_1-post.nii.gz\n",
      "\n",
      "\n",
      " 3  subset of Validation samples are:\n",
      "\n",
      "Pat_109_1-post.nii.gz\tPat_114_1-post.nii.gz\tPat_118_1-post.nii.gz\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " 100 Training samples are:\n",
      "\n",
      "Pat_100_1-post.nii.gz\tPat_101_1-post.nii.gz\tPat_102_1-post.nii.gz\tPat_103_1-post.nii.gz\tPat_107_1-post.nii.gz\tPat_108_1-post.nii.gz\tPat_10_1-post.nii.gz\tPat_110_1-post.nii.gz\tPat_113_1-post.nii.gz\tPat_115_1-post.nii.gz\tPat_116_1-post.nii.gz\tPat_117_1-post.nii.gz\tPat_121_1-post.nii.gz\tPat_122_1-post.nii.gz\tPat_123_1-post.nii.gz\tPat_125_1-post.nii.gz\tPat_126_1-post.nii.gz\tPat_127_1-post.nii.gz\tPat_128_1-post.nii.gz\tPat_12_1-post.nii.gz\tPat_132_1-post.nii.gz\tPat_135_1-post.nii.gz\tPat_13_1-post.nii.gz\tPat_140_1-post.nii.gz\tPat_141_1-post.nii.gz\tPat_142_1-post.nii.gz\tPat_143_1-post.nii.gz\tPat_14_1-post.nii.gz\tPat_15_1-post.nii.gz\tPat_16_1-post.nii.gz\tPat_17_1-post.nii.gz\tPat_18_1-post.nii.gz\tPat_19_1-post.nii.gz\tPat_1_1-post.nii.gz\tPat_20_1-post.nii.gz\tPat_21_1-post.nii.gz\tPat_23_1-post.nii.gz\tPat_27_1-post.nii.gz\tPat_28_1-post.nii.gz\tPat_29_1-post.nii.gz\tPat_2_1-post.nii.gz\tPat_30_1-post.nii.gz\tPat_31_1-post.nii.gz\tPat_32_1-post.nii.gz\tPat_33_1-post.nii.gz\tPat_35_1-post.nii.gz\tPat_36_1-post.nii.gz\tPat_37_1-post.nii.gz\tPat_38_1-post.nii.gz\tPat_39_1-post.nii.gz\tPat_3_1-post.nii.gz\tPat_41_1-post.nii.gz\tPat_42_1-post.nii.gz\tPat_43_1-post.nii.gz\tPat_44_1-post.nii.gz\tPat_45_1-post.nii.gz\tPat_46_1-post.nii.gz\tPat_47_1-post.nii.gz\tPat_49_1-post.nii.gz\tPat_4_1-post.nii.gz\tPat_50_1-post.nii.gz\tPat_51_1-post.nii.gz\tPat_52_1-post.nii.gz\tPat_53_1-post.nii.gz\tPat_54_1-post.nii.gz\tPat_55_1-post.nii.gz\tPat_56_1-post.nii.gz\tPat_57_1-post.nii.gz\tPat_58_1-post.nii.gz\tPat_59_1-post.nii.gz\tPat_5_1-post.nii.gz\tPat_61_1-post.nii.gz\tPat_62_1-post.nii.gz\tPat_63_1-post.nii.gz\tPat_67_1-post.nii.gz\tPat_68_1-post.nii.gz\tPat_69_1-post.nii.gz\tPat_71_1-post.nii.gz\tPat_73_1-post.nii.gz\tPat_74_1-post.nii.gz\tPat_75_1-post.nii.gz\tPat_77_1-post.nii.gz\tPat_78_1-post.nii.gz\tPat_79_1-post.nii.gz\tPat_7_1-post.nii.gz\tPat_80_1-post.nii.gz\tPat_82_1-post.nii.gz\tPat_84_1-post.nii.gz\tPat_85_1-post.nii.gz\tPat_86_1-post.nii.gz\tPat_87_1-post.nii.gz\tPat_89_1-post.nii.gz\tPat_8_1-post.nii.gz\tPat_90_1-post.nii.gz\tPat_91_1-post.nii.gz\tPat_92_1-post.nii.gz\tPat_93_1-post.nii.gz\tPat_95_1-post.nii.gz\tPat_98_1-post.nii.gz\tPat_9_1-post.nii.gz\n",
      "\n",
      "\n",
      " 5  subset of Validation samples are:\n",
      "\n",
      "Pat_100_1-post.nii.gz\tPat_101_1-post.nii.gz\tPat_102_1-post.nii.gz\tPat_103_1-post.nii.gz\tPat_107_1-post.nii.gz\n"
     ]
    }
   ],
   "source": [
    "## Deviding all samples into two groups: Training and Validation datasets randomly\n",
    "\n",
    "sample_val = random.sample(sample_all, 24)\n",
    "sample_val.sort()\n",
    "print(len(sample_val), \"Validation samples are:\\n\")\n",
    "print(*sample_val, sep='\\t')\n",
    "\n",
    "sample_val_subset = sample_val [0:3]\n",
    "print(\"\\n\\n\", len(sample_val_subset), \" subset of Validation samples are:\\n\")\n",
    "print(*sample_val_subset, sep='\\t')\n",
    "\n",
    "sample_train = [x for x in sample_all if x not in sample_val]\n",
    "sample_train.sort()\n",
    "print(\"\\n--------------------------------------------------------------------------------\\n\", len(sample_train), \"Training samples are:\\n\")\n",
    "print(*sample_train, sep='\\t')\n",
    "\n",
    "sample_train_subset = sample_train [0:5]\n",
    "print(\"\\n\\n\", len(sample_train_subset), \" subset of Validation samples are:\\n\")\n",
    "print(*sample_train_subset, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset: (5, 182, 218, 182, 1)\n"
     ]
    }
   ],
   "source": [
    "## Combine all Training dateset together\n",
    "\n",
    "data_combined = np.zeros((1, 182, 218, 182, 1))\n",
    "\n",
    "sample_train = sample_train_subset            # To test in CPU based computer\n",
    "\n",
    "for sample in sample_train_subset:\n",
    "#     print(sample)\n",
    "    img = nib.load(sample)\n",
    "    img_np = np.asanyarray(img.dataobj)         # Convert data into Numpy array format\n",
    "    data_64 = np.float64(img_np)                # Convert numpayed data back into the original float64\n",
    "    data_reshape = np.reshape(data_64, (1, 182, 218, 182, 1))\n",
    "    data_combined = np.concatenate((data_combined, data_reshape), axis=0)\n",
    "data_combined = np.delete(data_combined, 0, axis=0)\n",
    "print(\"Shape of training dataset:\", data_combined.shape)\n",
    "train_data = data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Showing one or all Training samples in three dimension (one middle slice per each dimension)\n",
    "\n",
    "def show_slices(slices):\n",
    "    fig, axes = plt.subplots(1, len(slices), figsize=(10,5))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"hot\", origin=\"upper\") # hot, Greys, gray\n",
    "for m in range(data_combined.shape[0]):\n",
    "    slice_0 = data_combined[m, 91, :, :, 0]\n",
    "    slice_1 = data_combined[m, :, 109, :, 0]\n",
    "    slice_2 = data_combined[m, :, :, 91, 0]\n",
    "#     show_slices([slice_0, slice_1, slice_2])\n",
    "    plt.suptitle(sample_train[m], x=0.5, y=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of a sample of original training data:\tMin= -0.0010085757821798325 ,   Max= 0.9992789626121521 \n",
      "\n",
      "Range of a sample of original data:\t\tMin= 0.0 ,   Max= 0.9992789626121521 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Rescale Training dataset into [0:1]\n",
    "\n",
    "sample_spec = 2      # A specific sample to show the min and max of that \n",
    "print(\"Range of a sample of original training data:\\tMin=\", np.min(train_data[sample_spec, :, :, :, 0]), \",   Max=\", np.max(train_data[sample_spec, :, :, :, 0]), \"\\n\")\n",
    "for m in range(train_data.shape[0]):\n",
    "    for x in range(train_data.shape[1]):\n",
    "        for y in range(train_data.shape[2]):\n",
    "            for z in range(train_data.shape[3]):\n",
    "                if train_data[m,x,y,z] <= 0: train_data[m,x,y,z] = 0\n",
    "                if train_data[m,x,y,z] > 1: train_data[m,x,y,z] = 1\n",
    "print(\"Range of a sample of original data:\\t\\tMin=\", np.min(train_data[sample_spec, :, :, :, 0]), \",   Max=\", np.max(train_data[sample_spec, :, :, :, 0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation dataset: (3, 182, 218, 182, 1)\n"
     ]
    }
   ],
   "source": [
    "## Combine all Validation dateset together\n",
    "\n",
    "data_combined = np.zeros((1, 182, 218, 182, 1))\n",
    "\n",
    "sample_val = sample_val_subset            # To test in CPU based computer\n",
    "\n",
    "for sample in sample_val:\n",
    "#     print(sample)\n",
    "    img = nib.load(sample)\n",
    "    img_np = np.asanyarray(img.dataobj)         # Convert data into Numpy array format\n",
    "    data_64 = np.float64(img_np)                # Convert numpayed data back into the original float64\n",
    "    data_reshape = np.reshape(data_64, (1, 182, 218, 182, 1))\n",
    "    data_combined = np.concatenate((data_combined, data_reshape), axis=0)\n",
    "data_combined = np.delete(data_combined, 0, axis=0)\n",
    "print(\"Shape of validation dataset:\", data_combined.shape)\n",
    "valid_data = data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Showing one or all Validation samples in three dimension (one middle slice per each dimension)\n",
    "\n",
    "for m in range(data_combined.shape[0]):\n",
    "    slice_0 = data_combined[m, 91, :, :, 0]\n",
    "    slice_1 = data_combined[m, :, 109, :, 0]\n",
    "    slice_2 = data_combined[m, :, :, 91, 0]\n",
    "#     show_slices([slice_0, slice_1, slice_2])\n",
    "    plt.suptitle(sample_val[m], x=0.5, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of a sample of original validation data:\tMin= 0.0 ,   Max= 0.9882298111915588 \n",
      "\n",
      "Range of a sample of original data:\t\tMin= 0.0 ,   Max= 0.9882298111915588 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Rescale Validation dataset into [0:1]\n",
    "\n",
    "sample_spec = 2      # A specific sample to show the min and max of that \n",
    "\n",
    "print(\"Range of a sample of original validation data:\\tMin=\", np.min(valid_data[sample_spec, :, :, :, 0]), \",   Max=\", np.max(valid_data[sample_spec, :, :, :, 0]), \"\\n\")\n",
    "for m in range(valid_data.shape[0]):\n",
    "    for x in range(valid_data.shape[1]):\n",
    "        for y in range(valid_data.shape[2]):\n",
    "            for z in range(valid_data.shape[3]):\n",
    "                if valid_data[m,x,y,z] <= 0: valid_data[m,x,y,z] = 0\n",
    "                if valid_data[m,x,y,z] > 1: valid_data[m,x,y,z] = 1\n",
    "print(\"Range of a sample of original data:\\t\\tMin=\", np.min(valid_data[sample_spec, :, :, :, 0]), \",   Max=\", np.max(valid_data[sample_spec, :, :, :, 0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for shape and one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 182, 218, 182, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69851756]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[[1],[100],[100],[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 182, 218, 182, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73677564]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[[1],[100],[100],[100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shossein/anaconda3/envs/m36/lib/python3.6/site-packages/nibabel/tests/data/DTI\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory is changed into:\n",
      " /Users/shossein/GitHub/DTI_Clustering \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dirpath = os.chdir(\"/Users/shossein/GitHub/DTI_Clustering\")\n",
    "print(\"\\nDirectory is changed into:\\n\", os.getcwd(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data reshaped to: (5, 7221032)  and valid_data reshaped to : (3, 7221032)\n"
     ]
    }
   ],
   "source": [
    "train_data_reshaped = train_data.reshape(5,7221032)\n",
    "valid_data_reshaped = valid_data.reshape(3,7221032)\n",
    "print(\"train_data reshaped to:\", train_data_reshaped.shape, \" and valid_data reshaped to :\", valid_data_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"train_data.csv\", train_data_reshaped, delimiter=\",\")\n",
    "np.savetxt(\"valid_data.csv\", valid_data_reshaped, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loaded = np.loadtxt(\"train_data.csv\", delimiter=\",\").reshape((5, 182, 218, 182, 1))\n",
    "valid_data_loaded = np.loadtxt(\"valid_data.csv\", delimiter=\",\").reshape((3, 182, 218, 182, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 182, 218, 182, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69851756]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loaded[[1],[100],[100],[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 182, 218, 182, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73677564]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_loaded[[1],[100],[100],[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert valid_data[[1],[100],[100],[100]] == valid_data_loaded[[1],[100],[100],[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_data[[1],[100],[100],[100]] == train_data_loaded[[1],[100],[100],[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_data.all() == train_data_loaded.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert valid_data.all() == valid_data_loaded.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input-layer shape: [32, 182, 218, 182, 1]\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 182, 218, 182, 1) 0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv3D)               (None, 182, 218, 182, 16) 448       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 14, 17, 14, 16)    0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv3D)               (None, 14, 17, 14, 8)     3464      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 2, 3, 2, 8)        0         \n",
      "_________________________________________________________________\n",
      "DeConv3 (Conv3DTranspose)    (None, 2, 3, 2, 4)        868       \n",
      "_________________________________________________________________\n",
      "up_sampling3d (UpSampling3D) (None, 14, 18, 14, 4)     0         \n",
      "_________________________________________________________________\n",
      "DeConv4 (Conv3DTranspose)    (None, 14, 18, 14, 8)     872       \n",
      "_________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3 (None, 182, 216, 182, 8)  0         \n",
      "_________________________________________________________________\n",
      "DeConv5 (Conv3DTranspose)    (None, 182, 218, 182, 1)  25        \n",
      "=================================================================\n",
      "Total params: 5,677\n",
      "Trainable params: 5,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Define parameters:\n",
    "\n",
    "IMAGE_HEIGHT = train_data_loaded.shape[1]\n",
    "IMAGE_WIDTH = train_data_loaded.shape[2]\n",
    "IMAGE_DEPTH = train_data_loaded.shape[3]\n",
    "batch_size = 32\n",
    "data_shape = [1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "input_shape = [batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "print(\"input-layer shape:\", input_shape)\n",
    "\n",
    "## Encoder\n",
    "input_img = Input(shape=(182, 218, 182, 1))\n",
    "x = Conv3D(filters=16, kernel_size=(3, 3, 3), padding='same', activation='relu', name='Conv1')(input_img)\n",
    "x = MaxPooling3D(pool_size=(13, 13, 13), padding='same')(x)\n",
    "x = Conv3D(filters=8, kernel_size=(3, 3, 3), padding='same', activation='relu', name='Conv2')(x)\n",
    "encoded = MaxPooling3D(pool_size=(7, 7, 7), padding='same')(x)\n",
    "## at this point the representation is (2, 3, 2, 8) i.e. 96-dimensional instead of 7,221,032\n",
    "\n",
    "## Decoder\n",
    "x = Conv3DTranspose(filters=4, kernel_size=(3, 3, 3), padding='same', activation='relu', name='DeConv3')(encoded)\n",
    "x = UpSampling3D(size=(7, 6, 7))(x)\n",
    "x = Conv3DTranspose(filters=8, kernel_size=(3, 3, 3), padding='same', activation='relu', name='DeConv4')(x)\n",
    "x = UpSampling3D(size=(13, 12, 13))(x)\n",
    "decoded = Conv3DTranspose(filters=1, kernel_size=(1, 3, 1), padding='valid', activation='sigmoid', name='DeConv5')(x)\n",
    "\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.0000e+00 - accuracy: 0.3087 - val_loss: 0.0000e+00 - val_accuracy: 0.3026\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0000e+00 - accuracy: 0.3087 - val_loss: 0.0000e+00 - val_accuracy: 0.3026\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0000e+00 - accuracy: 0.3087 - val_loss: 0.0000e+00 - val_accuracy: 0.3026\n"
     ]
    }
   ],
   "source": [
    "## Tensorboard\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "# logdir = os.path.join(\"CAE_logs\", datetime.datetime.now().strftime(\"%Y_%m_%d____%H_%M_%S\"))\n",
    "# tb_callback = TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "## Midel Fit\n",
    "# autoencoder.fit(train_data, train_data, epochs=3, batch_size=batch_size, shuffle=True, validation_data=(valid_data, valid_data), callbacks=[tb_callback], verbose=1)\n",
    "autoencoder.fit(train_data_loaded, train_data_loaded, epochs=3, batch_size=batch_size, shuffle=True, validation_data=(valid_data_loaded, valid_data_loaded), verbose=1)\n",
    "autoencoder.save_weights(\"CAE_weights.hdf5\")\n",
    "# autoencoder.load_weights(\"CAE_weights.hdf5\")     # loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=CAE_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
