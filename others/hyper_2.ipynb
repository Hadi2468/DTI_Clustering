{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some helpful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"    Version control\\n------------------------\")\n",
    "import os     # operating system interfaces\n",
    "import fnmatch\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np;              print(\"Numpy\\t\\t\", np.__version__)\n",
    "import matplotlib as mpl;        print(\"matplotlib\\t\", mpl.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib;           print(\"NiBabel\\t\\t {}\".format(nib.__version__))\n",
    "from nibabel.testing import data_path\n",
    "import math\n",
    "import pandas as pd;             print(\"Pandas\\t\\t {}\".format(pd.__version__))\n",
    "import sys\n",
    "import imageio;                  print(\"imageio\\t\\t {}\".format(imageio.__version__))\n",
    "import h5py;                     print(\"H5py\\t\\t {}\".format(h5py.__version__))\n",
    "import sklearn;                  print(\"Scikit-learn\\t {}\".format(sklearn.__version__))\n",
    "import skimage;                  print(\"Scikit-image\\t {}\".format(skimage.__version__))\n",
    "import tensorflow as tf;         print(\"TensorFlow\\t {}\".format(tf.__version__))\n",
    "import keras;                    print(\"Keras\\t\\t {}\".format(keras.__version__))\n",
    "from tensorflow.keras import models, Input, Model\n",
    "from tensorflow.keras.layers import Dense, Conv3D, Conv3DTranspose, MaxPooling3D, UpSampling3D\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset: Training and Validation Datasets (randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading dataset from path directory\n",
    "print(os.getcwd())     # '/Users/shossein/GitHub/DTI_Clustering'\n",
    "\n",
    "## load train data\n",
    "sample_train_subset = np.loadtxt(\"train_100_sample_name.csv\", dtype=str, delimiter=\",\")\n",
    "train_data = np.load('train_100.npy').reshape(100,182,218,182,1)\n",
    "print('train_data shape is {}'.format(train_data.shape))\n",
    "\n",
    "## load validation data\n",
    "sample_val_subset = np.loadtxt(\"valid_24_sample_name.csv\", dtype=str, delimiter=\",\")\n",
    "valid_data = np.load('valid_24.npy').reshape(24,182,218,182,1)\n",
    "print('valid_data shape is {}'.format(valid_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", len(sample_train_subset), \" subset of train samples are:\\n\")\n",
    "print(*sample_train_subset, sep='\\t')\n",
    "print(\"\\n--------------------------------------------------------------------------------\\n\")\n",
    "print(\"There are\", len(sample_val_subset), \" subset of Validation samples are:\\n\")\n",
    "print(*sample_val_subset, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing one or all Training samples in three dimension (one middle slice per each dimension)\n",
    "\n",
    "def show_slices(slices):\n",
    "    fig, axes = plt.subplots(1, len(slices), figsize=(10,5))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"hot\", origin=\"upper\") # hot, Greys, gray\n",
    "        \n",
    "# for m in range(train_data.shape[0]):\n",
    "for m in range(1):\n",
    "    slice_0 = train_data[m, 91, :, :, 0]\n",
    "    slice_1 = train_data[m, :, 109, :, 0]\n",
    "    slice_2 = train_data[m, :, :, 91, 0]\n",
    "    show_slices([slice_0, slice_1, slice_2])\n",
    "    plt.suptitle(sample_train_subset[m], x=0.5, y=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing one or all Validation samples in three dimension (one middle slice per each dimension)\n",
    "\n",
    "# for m in range(valid_data.shape[0]):\n",
    "for m in range(1):\n",
    "    slice_0 = valid_data[m, 91, :, :, 0]\n",
    "    slice_1 = valid_data[m, :, 109, :, 0]\n",
    "    slice_2 = valid_data[m, :, :, 91, 0]\n",
    "    show_slices([slice_0, slice_1, slice_2])\n",
    "    plt.suptitle(sample_val_subset[m], x=0.5, y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters:\n",
    "\n",
    "IMAGE_HEIGHT = train_data.shape[1]\n",
    "IMAGE_WIDTH = train_data.shape[2]\n",
    "IMAGE_DEPTH = train_data.shape[3]\n",
    "batch_size = 1\n",
    "data_shape = [1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "input_shape = [batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH, 1]\n",
    "print(\"input-layer shape:\", input_shape)\n",
    "\n",
    "## Encoder\n",
    "input_img = Input(shape=(182, 218, 182, 1))\n",
    "x = Conv3D(filters=16, kernel_size=(3, 3, 3), padding='same', activation='relu', name='Conv1')(input_img)\n",
    "x = MaxPooling3D(pool_size=(13, 13, 13), padding='same')(x)\n",
    "x = Conv3D(filters=8, kernel_size=(3, 3, 3), padding='same', activation='relu', name='Conv2')(x)\n",
    "encoded = MaxPooling3D(pool_size=(7, 7, 7), padding='same')(x)\n",
    "## at this point the representation is (2, 3, 2, 8) i.e. 96-dimensional instead of 7,221,032\n",
    "\n",
    "## Decoder\n",
    "x = Conv3DTranspose(filters=4, kernel_size=(3, 3, 3), padding='same', activation='relu', name='DeConv3')(encoded)\n",
    "x = UpSampling3D(size=(7, 6, 7))(x)\n",
    "x = Conv3DTranspose(filters=8, kernel_size=(3, 3, 3), padding='same', activation='relu', name='DeConv4')(x)\n",
    "x = UpSampling3D(size=(13, 12, 13))(x)\n",
    "decoded = Conv3DTranspose(filters=1, kernel_size=(1, 3, 1), padding='valid', activation='sigmoid', name='DeConv5')(x)\n",
    "\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "# autoencoder.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])     # optimizer=rmsprop,     loss=binary_crossentropy\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorboard\n",
    "\n",
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "logdir = os.path.join(\"CAE_logs\", datetime.datetime.now().strftime(\"%Y_%m_%d____%H_%M_%S\"))\n",
    "tb_callback = TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start time:\n",
    "from datetime import datetime\n",
    "start_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\nStart Time =\", start_time, \"\\n\")\n",
    "\n",
    "## Midel Fit\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\n",
    "autoencoder.fit(train_data, train_data, epochs=3, batch_size=batch_size, shuffle=True, validation_data=(valid_data, valid_data), callbacks=[tb_callback], verbose=1)\n",
    "autoencoder.save_weights(\"CAE_weights.hdf5\")\n",
    "# autoencoder.load_weights(\"CAE_weights.hdf5\")     # loading weights\n",
    "\n",
    "# End time:\n",
    "from datetime import datetime\n",
    "end_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\nEnd Time =\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=CAE_logs       # http://localhost:6006/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
