{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Version control\n",
      "------------------------\n",
      "Numpy\t\t 1.19.4\n",
      "matplotlib\t 3.3.3\n",
      "Pandas\t\t 1.1.4\n"
     ]
    }
   ],
   "source": [
    "## Import some helpful libraries\n",
    "print(\"    Version control\\n------------------------\")\n",
    "import os, fnmatch, random, math, sys, datetime\n",
    "from pathlib import Path\n",
    "import numpy as np;              print(\"Numpy\\t\\t\", np.__version__)\n",
    "import matplotlib as mpl;        print(\"matplotlib\\t\", mpl.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;             print(\"Pandas\\t\\t {}\".format(pd.__version__))\n",
    "from IPython.display import Image\n",
    "\n",
    "# import nibabel as nib;           print(\"NiBabel\\t\\t {}\".format(nib.__version__))\n",
    "# from nibabel.testing import data_path\n",
    "# import imageio;                  print(\"imageio\\t\\t {}\".format(imageio.__version__))\n",
    "# import h5py;                     print(\"H5py\\t\\t {}\".format(h5py.__version__))\n",
    "# import sklearn;                  print(\"Scikit-learn\\t {}\".format(sklearn.__version__))\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from scipy.cluster import hierarchy\n",
    "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# import skimage;                  print(\"Scikit-image\\t {}\".format(skimage.__version__))\n",
    "# import tensorflow as tf;         print(\"TensorFlow\\t {}\".format(tf.__version__))\n",
    "# import keras;                    print(\"Keras\\t\\t {}\".format(keras.__version__))\n",
    "# import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras import models, Input, Model\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Reshape, BatchNormalization, Conv3D, MaxPooling3D, UpSampling3D, ZeroPadding3D\n",
    "# from tensorflow.keras.activations import relu, sigmoid\n",
    "# from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "# from tensorflow.keras.initializers import *\n",
    "# from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "# from keras.engine.topology import Layer, InputSpec\n",
    "# from keras.optimizers import SGD\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# %load_ext tensorboard       \n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuroCognitive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Loading original Neurocognitive score data\n",
    "\n",
    "# score_df = pd.read_excel('./Data/Neurocognitive_Scores/Scores_original.xlsx')\n",
    "# df = score_df\n",
    "# for p in range (len(df)):\n",
    "#     if df.ID[p][5] == '_': \n",
    "#         df.ID[p] = df.ID[p][0:4] + '00' + df.ID[p][4:]\n",
    "#     elif df.ID[p][6] == '_': \n",
    "#         df.ID[p] = df.ID[p][0:4] + '0' + df.ID[p][4:]\n",
    "# print('\\033[1m There are totally {} records\\n'.format(len(df)))\n",
    "\n",
    "# ## Extract the Baseline_Phase and save\n",
    "# df = df.rename(columns={'Phase Description':'Phase_Description', 'GIA SS':'GIA_SS', 'Proc Spd SS':'Proc_Spd_SS', 'Working Mem SS':'Working_Mem_SS', 'Broad Attn SS':'Broad_Attn_SS'})\n",
    "# df.index = df[df.columns[0]]\n",
    "# df = df.drop(['ID'], axis=1)\n",
    "# df = df.drop(['Date'], axis=1)\n",
    "# df_base = df[(df.Phase_Description == 'Baseline phase') | (df.Phase_Description == 'Baseline Phase')]\n",
    "# df_base.Phase_Description = 'baseline_phase'\n",
    "# print('\\033[1m There are {} records of baseline_phasse\\n'.format(len(df_base)))\n",
    "# df_base.to_csv((r'./Data/Neurocognitive_Scores/baseline.csv'))\n",
    "\n",
    "# ## Extract the 36_months_Phase and save\n",
    "# df_36 = df[(df.Phase_Description == 'Post Dx 36 months')]# | (df.Phase_Description == 'Baseline Phase')]\n",
    "# df_36.Phase_Description = '36_months'\n",
    "# print('\\033[1m There are {} records of 36_months\\n'.format(len(df_36)))\n",
    "# df_36.to_csv((r'./Data/Neurocognitive_Scores/m36.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generating Scores for Baseline\n",
    "\n",
    "# df_base = pd.read_csv('./Data/Neurocognitive_Scores/baseline.csv')\n",
    "# df_base.index = df_base.ID\n",
    "\n",
    "# ## Baseline_phasse & GIA_Score\n",
    "# df_base_GIA = df_base.GIA_SS\n",
    "# df_base_GIA = df_base_GIA.dropna()   # remove NaN\n",
    "# df_base_GIA = df_base_GIA.astype(int)\n",
    "# # df_base_GIA.to_csv((r'./Data/Neurocognitive_Scores/baseline_GIA.csv'))\n",
    "\n",
    "# ## Baseline_phasse & Process_Spead_Score\n",
    "# df_base_Proc_Spd = df_base.Proc_Spd_SS\n",
    "# df_base_Proc_Spd = df_base_Proc_Spd.dropna()   # remove NaN\n",
    "# df_base_Proc_Spd = df_base_Proc_Spd.astype(int)\n",
    "# # df_base_Proc_Spd.to_csv((r'./Data/Neurocognitive_Scores/baseline_Proc_Spd.csv'))\n",
    "\n",
    "# ## Baseline_phasse & Working_Memory_Score\n",
    "# df_base_Working_Mem = df_base.Working_Mem_SS\n",
    "# df_base_Working_Mem = df_base_Working_Mem.dropna()   # remove NaN\n",
    "# df_base_Working_Mem = df_base_Working_Mem.astype(int)\n",
    "# # df_base_Working_Mem.to_csv((r'./Data/Neurocognitive_Scores/baseline_Working_Mem.csv'))\n",
    "\n",
    "# ## Baseline_phasse & Broad_Attention_Score\n",
    "# df_base_Broad_Attn = df_base.Broad_Attn_SS\n",
    "# df_base_Broad_Attn = df_base_Broad_Attn.dropna()   # remove NaN\n",
    "# df_base_Broad_Attn = df_base_Broad_Attn.astype(int)\n",
    "# # df_base_Broad_Attn.to_csv((r'./Data/Neurocognitive_Scores/baseline_Broad_Attn.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generating Scores for 36_Months\n",
    "\n",
    "# df_m36 = pd.read_csv('./Data/Neurocognitive_Scores/m36.csv')\n",
    "# df_m36.index = df_m36.ID\n",
    "\n",
    "# ## Month-36 & GIA_Score\n",
    "# df_m36_GIA = df_m36.GIA_SS\n",
    "# df_m36_GIA = df_m36_GIA.dropna()   # remove NaN\n",
    "# df_m36_GIA = df_m36_GIA.astype(int)\n",
    "# # df_m36_GIA.to_csv((r'./Data/Neurocognitive_Scores/m36_GIA.csv'))\n",
    "\n",
    "# ## Month-36 & Process_Spead_Score\n",
    "# df_m36_Proc_Spd = df_m36.Proc_Spd_SS\n",
    "# df_m36_Proc_Spd = df_m36_Proc_Spd.dropna()   # remove NaN\n",
    "# df_m36_Proc_Spd = df_m36_Proc_Spd.astype(int)\n",
    "# # df_m36_Proc_Spd.to_csv((r'./Data/Neurocognitive_Scores/m36_Proc_Spd.csv'))\n",
    "\n",
    "# ## Month-36 & Working_Memory_Score\n",
    "# df_m36_Working_Mem = df_m36.Working_Mem_SS\n",
    "# df_m36_Working_Mem = df_m36_Working_Mem.dropna()   # remove NaN\n",
    "# df_m36_Working_Mem = df_m36_Working_Mem.astype(int)\n",
    "# # df_m36_Working_Mem.to_csv((r'./Data/Neurocognitive_Scores/m36_Working_Mem.csv'))\n",
    "\n",
    "# ## Month-36 & Broad_Attention_Score\n",
    "# df_m36_Broad_Attn = df_m36.Broad_Attn_SS\n",
    "# df_m36_Broad_Attn = df_m36_Broad_Attn.dropna()   # remove NaN\n",
    "# df_m36_Broad_Attn = df_m36_Broad_Attn.astype(int)\n",
    "# # df_m36_Broad_Attn.to_csv((r'./Data/Neurocognitive_Scores/m36_Broad_Attn.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generating Scores for (36_Months - Baseline_Phase)\n",
    "\n",
    "# ## m36-base & Process_GIA_Score\n",
    "# df_m36_join_base_GIA = pd.merge(df_m36_GIA, df_base_GIA, how='inner', on=['ID'])\n",
    "# df_m36_join_base_GIA['GIA_SS'] = df_m36_join_base_GIA.GIA_SS_x - df_m36_join_base_GIA.GIA_SS_y \n",
    "# df_m36_base_GIA = df_m36_join_base_GIA.drop(['GIA_SS_x', 'GIA_SS_y'], axis=1)\n",
    "# # df_m36_base_GIA.to_csv((r'./Data/Neurocognitive_Scores/m36_base_GIA.csv'), index=False)\n",
    "\n",
    "# ## m36-base & Process_Spead_Score\n",
    "# df_base_Proc_Spd = pd.read_csv('./Data/Neurocognitive_Scores/baseline_Proc_Spd.csv')\n",
    "# df_m36_Proc_Spd = pd.read_csv('./Data/Neurocognitive_Scores/m36_Proc_Spd.csv')\n",
    "# df_m36_join_base_Proc_Spd = pd.merge(df_m36_Proc_Spd, df_base_Proc_Spd, how='inner', on=['ID'])\n",
    "# df_m36_join_base_Proc_Spd['Proc_Spd_SS'] = df_m36_join_base_Proc_Spd.Proc_Spd_SS_x - df_m36_join_base_Proc_Spd.Proc_Spd_SS_y \n",
    "# df_m36_base_Proc_Spd = df_m36_join_base_Proc_Spd.drop(['Proc_Spd_SS_x', 'Proc_Spd_SS_y'], axis=1)\n",
    "# # df_m36_base_Proc_Spd.to_csv((r'./Data/Neurocognitive_Scores/m36_base_Proc_Spd.csv'), index=False)\n",
    "\n",
    "# ## m36-base & Working_Memory_Score\n",
    "# df_base_Working_Mem = pd.read_csv('./Data/Neurocognitive_Scores/baseline_Working_Mem.csv')\n",
    "# df_m36_Working_Mem = pd.read_csv('./Data/Neurocognitive_Scores/m36_Working_Mem.csv')\n",
    "# df_m36_join_base_Working_Mem = pd.merge(df_m36_Working_Mem, df_base_Working_Mem, how='inner', on=['ID'])\n",
    "# df_m36_join_base_Working_Mem['Working_Mem_SS'] = df_m36_join_base_Working_Mem.Working_Mem_SS_x - df_m36_join_base_Working_Mem.Working_Mem_SS_y \n",
    "# df_m36_base_Working_Mem = df_m36_join_base_Working_Mem.drop(['Working_Mem_SS_x', 'Working_Mem_SS_y'], axis=1)\n",
    "# # df_m36_base_Working_Mem.to_csv((r'./Data/Neurocognitive_Scores/m36_base_Working_Mem.csv'), index=False)\n",
    "\n",
    "# ## m36-base & Broad_Attention_Score\n",
    "# df_base_Broad_Attn = pd.read_csv('./Data/Neurocognitive_Scores/baseline_Broad_Attn.csv')\n",
    "# df_m36_Broad_Attn = pd.read_csv('./Data/Neurocognitive_Scores/m36_Broad_Attn.csv')\n",
    "# df_m36_join_base_Broad_Attn = pd.merge(df_m36_Broad_Attn, df_base_Broad_Attn, how='inner', on=['ID'])\n",
    "# df_m36_join_base_Broad_Attn['Broad_Attn_SS'] = df_m36_join_base_Broad_Attn.Broad_Attn_SS_x - df_m36_join_base_Broad_Attn.Broad_Attn_SS_y \n",
    "# df_m36_base_Broad_Attn = df_m36_join_base_Broad_Attn.drop(['Broad_Attn_SS_x', 'Broad_Attn_SS_y'], axis=1)\n",
    "# # df_m36_base_Broad_Attn.to_csv((r'./Data/Neurocognitive_Scores/m36_base_Broad_Attn.csv'), index=False)\n",
    "\n",
    "# ## m36-base for All Scores\n",
    "# df_m36base_all_scores = pd.merge(cluster_m36base_GIA, cluster_m36base_Proc_Spd, how='outer', on=['ID', 'class_e500', 'class_e1500'])\n",
    "# df_m36base_all_scores = pd.merge(df_m36base_all_scores, cluster_m36base_Working_Mem, how='outer', on=['ID', 'class_e500', 'class_e1500'])\n",
    "# df_m36base_all_scores = pd.merge(df_m36base_all_scores, cluster_m36base_Broad_Attn, how='outer', on=['ID', 'class_e500', 'class_e1500'])\n",
    "# df_m36base_all_scores = df_m36base_all_scores.drop(['class_e500', 'class_e1500'], axis=1)\n",
    "# # df_m36base_all_scores.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/df_m36base_all_scores.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>GIA_SS</th>\n",
       "      <th>Proc_Spd_SS</th>\n",
       "      <th>Working_Mem_SS</th>\n",
       "      <th>Broad_Attn_SS</th>\n",
       "      <th>Cluster_SS_e500</th>\n",
       "      <th>Cluster_SS_e1500</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>...</th>\n",
       "      <th>X36854</th>\n",
       "      <th>X36855</th>\n",
       "      <th>X36856</th>\n",
       "      <th>X36857</th>\n",
       "      <th>X36858</th>\n",
       "      <th>X36859</th>\n",
       "      <th>X36860</th>\n",
       "      <th>X36861</th>\n",
       "      <th>X36862</th>\n",
       "      <th>X36863</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pat_001_1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.124379</td>\n",
       "      <td>1.648069</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pat_002_1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.124379</td>\n",
       "      <td>1.648069</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pat_004_1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.124379</td>\n",
       "      <td>1.648069</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pat_007_1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.124379</td>\n",
       "      <td>1.648069</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pat_011_1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.124379</td>\n",
       "      <td>1.648069</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Pat_140_1</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.124379</td>\n",
       "      <td>1.648069</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Pat_087_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.110988</td>\n",
       "      <td>1.480344</td>\n",
       "      <td>-0.089622</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Pat_109_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.124379</td>\n",
       "      <td>1.648069</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Pat_005_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.124379</td>\n",
       "      <td>1.648069</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Pat_116_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.124379</td>\n",
       "      <td>1.648069</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.216609</td>\n",
       "      <td>-0.925276</td>\n",
       "      <td>-0.44029</td>\n",
       "      <td>-0.513905</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.225273</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.779455</td>\n",
       "      <td>-0.483009</td>\n",
       "      <td>-0.879515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 36871 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  GIA_SS  Proc_Spd_SS  Working_Mem_SS  Broad_Attn_SS  \\\n",
       "0   Pat_001_1    16.0         38.0            22.0           22.0   \n",
       "1   Pat_002_1    22.0          8.0             7.0            6.0   \n",
       "2   Pat_004_1     5.0        -12.0            -8.0           -9.0   \n",
       "3   Pat_007_1    19.0         23.0            13.0           16.0   \n",
       "4   Pat_011_1     6.0         -2.0            23.0           11.0   \n",
       "..        ...     ...          ...             ...            ...   \n",
       "57  Pat_140_1    -6.0         19.0            -7.0            2.0   \n",
       "58  Pat_087_1     NaN         57.0             NaN            NaN   \n",
       "59  Pat_109_1     NaN          4.0             NaN            NaN   \n",
       "60  Pat_005_1     NaN          NaN            17.0            NaN   \n",
       "61  Pat_116_1     NaN          NaN            13.0            NaN   \n",
       "\n",
       "    Cluster_SS_e500  Cluster_SS_e1500        X0        X1        X2  ...  \\\n",
       "0                 1                 1 -0.124379  1.648069 -0.074343  ...   \n",
       "1                 1                 1 -0.124379  1.648069 -0.074343  ...   \n",
       "2                 2                 1 -0.124379  1.648069 -0.074343  ...   \n",
       "3                 3                 3 -0.124379  1.648069 -0.074343  ...   \n",
       "4                 2                 2 -0.124379  1.648069 -0.074343  ...   \n",
       "..              ...               ...       ...       ...       ...  ...   \n",
       "57                2                 1 -0.124379  1.648069 -0.074343  ...   \n",
       "58                4                 4 -0.110988  1.480344 -0.089622  ...   \n",
       "59                3                 3 -0.124379  1.648069 -0.074343  ...   \n",
       "60                1                 2 -0.124379  1.648069 -0.074343  ...   \n",
       "61                2                 1 -0.124379  1.648069 -0.074343  ...   \n",
       "\n",
       "      X36854    X36855   X36856    X36857    X36858    X36859    X36860  \\\n",
       "0  -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "1  -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "2  -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "3  -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "4  -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "..       ...       ...      ...       ...       ...       ...       ...   \n",
       "57 -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "58 -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "59 -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "60 -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "61 -1.216609 -0.925276 -0.44029 -0.513905 -0.420753 -0.225273 -0.277623   \n",
       "\n",
       "      X36861    X36862    X36863  \n",
       "0  -0.779455 -0.483009 -0.879515  \n",
       "1  -0.779455 -0.483009 -0.879515  \n",
       "2  -0.779455 -0.483009 -0.879515  \n",
       "3  -0.779455 -0.483009 -0.879515  \n",
       "4  -0.779455 -0.483009 -0.879515  \n",
       "..       ...       ...       ...  \n",
       "57 -0.779455 -0.483009 -0.879515  \n",
       "58 -0.779455 -0.483009 -0.879515  \n",
       "59 -0.779455 -0.483009 -0.879515  \n",
       "60 -0.779455 -0.483009 -0.879515  \n",
       "61 -0.779455 -0.483009 -0.879515  \n",
       "\n",
       "[62 rows x 36871 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading LF_r and Neurocognitive Scores\n",
    "\n",
    "df_base = pd.read_csv('../Data/data_random_1/Neurocognitive/baseline.csv')\n",
    "# df_m36base_all_scores = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/df_m36base_all_scores.csv')\n",
    "# LF_r = pd.read_csv('../Data/data_random_1/LF/LF_r.csv')\n",
    "\n",
    "# LF_r_base = pd.merge(df_base, LF_r, how='inner', on=['ID'])\n",
    "# LF_r_m36base = pd.merge(df_m36base_all_scores, LF_r, how='inner', on=['ID'])\n",
    "\n",
    "# LF_r_base = LF_r_base.drop(['Phase_Description'], axis=1)\n",
    "# LF_r_base = LF_r_base.rename(columns={'epoch_500': 'Cluster_SS_e500', 'epoch_1500': 'Cluster_SS_e1500'})\n",
    "# LF_r_m36base = LF_r_m36base.rename(columns={'epoch_500': 'Cluster_SS_e500', 'epoch_1500': 'Cluster_SS_e1500'})\n",
    "\n",
    "# # LF_r_base.to_csv((r'../Data/data_random_1/LF_Joins/LF_r_base_total.csv'), index=False)\n",
    "# # LF_r_m36base.to_csv((r'../Data/data_random_1/LF_Joins/LF_r_m36base_total.csv'), index=False)\n",
    "\n",
    "LF_r_m36base = pd.read_csv(r'../Data/data_random_1/LF_Joins/LF_r_m36base_total.csv')\n",
    "LF_r_m36base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Joining of Cluster with Baseline\n",
    "\n",
    "# ## Cluster--*--GIA for base\n",
    "# df = pd.merge(df_clusters, df_base_GIA, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_base_GIA = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_base_GIA.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_base_GIA.csv'))\n",
    "\n",
    "# ## Cluster--*--ProcSpd for base\n",
    "# df = pd.merge(df_clusters, df_base_ProcSpd, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_base_ProcSpd = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_base_ProcSpd.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_base_Proc_Spd.csv'))\n",
    "\n",
    "# ## Cluster--*--WorkingMem for base\n",
    "# df = pd.merge(df_clusters, df_base_WorkingMem, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_base_WorkingMem = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_base_WorkingMem.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_base_Working_Mem.csv'))\n",
    "\n",
    "# ## Cluster--*--BroadAttn for base\n",
    "# df = pd.merge(df_clusters, df_base_BroadAttn, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_base_BroadAttn = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_base_BroadAttn.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_base_Broad_Attn.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Joining of Cluster with (36month - baseline)\n",
    "\n",
    "# ## Cluster--*--GIA for m36-base\n",
    "# df = pd.merge(df_clusters, df_m36_base_GIA, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_m36base_GIA = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_m36base_GIA.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_GIA.csv'))\n",
    "\n",
    "# ## Cluster--*--ProcSpd for m36-base\n",
    "# df = pd.merge(df_clusters, df_m36_base_ProcSpd, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_m36base_ProcSpd = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_m36base_ProcSpd.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Proc_Spd.csv'))\n",
    "\n",
    "# ## Cluster--*--WorkingMem for m36-base\n",
    "# df = pd.merge(df_clusters, df_m36_base_WorkingMem, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_m36base_WorkingMem = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_m36base_WorkingMem.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Working_Mem.csv'))\n",
    "\n",
    "# ## Cluster--*--BroadAttn for m36-base\n",
    "# df = pd.merge(df_clusters, df_m36_base_BroadAttn, how='inner', on=['ID'])\n",
    "# df.index = df[df.columns[0]]\n",
    "# df_cluster_join_m36base_BroadAttn = df.drop(['ID', 'same_clusters'], axis=1)\n",
    "# # df_cluster_join_m36base_BroadAttn.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Broad_Attn.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Joining of All scores with LF \n",
    "\n",
    "# ## Joining All Scores for Baseline-Phase\n",
    "# df_base_all_scores = pd.merge(df_clusters, df_base, how='inner', on=['ID'])\n",
    "# df_base_all_scores = df_base_all_scores.drop(['same_clusters', 'Phase_Description'], axis=1)\n",
    "# # df_base_all_scores.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/df_base_all_scores.csv'))\n",
    "\n",
    "# ## Joining All Scores for 36m-base\n",
    "# df_m36_all_scores = pd.merge(df_clusters, df_m36, how='inner', on=['ID'])\n",
    "# df_m36_all_scores = df_m36_all_scores.drop(['same_clusters', 'Phase_Description'], axis=1)\n",
    "# # df_m36_all_scores.to_csv((r'../Data/data_random_1/Neurocognitive_Joins/df_m36_all_scores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9635c17982b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# df_m36_all_scores  = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/df_m36_all_scores.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n\u001b[0m\u001b[1;32m     15\u001b[0m       'there are {} common records for GIA Score'.format(len(df_cluster_join_base_GIA)))\n\u001b[1;32m     16\u001b[0m print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_base' is not defined"
     ]
    }
   ],
   "source": [
    "df_cluster_join_base_GIA         = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_base_GIA.csv')\n",
    "df_cluster_join_base_Proc_Spd    = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_base_Proc_Spd.csv')\n",
    "df_cluster_join_base_Working_Mem = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_base_Working_Mem.csv')\n",
    "df_cluster_join_base_Broad_Attn  = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_base_Broad_Attn.csv')\n",
    "\n",
    "df_cluster_join_m36base_GIA         = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_GIA.csv')\n",
    "df_cluster_join_m36base_Proc_Spd    = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Proc_Spd.csv')\n",
    "df_cluster_join_m36base_Working_Mem = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Working_Mem.csv')\n",
    "df_cluster_join_m36base_Broad_Attn  = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/cluster_m36base_Broad_Attn.csv')\n",
    "\n",
    "# df_base_all_scores  = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/df_base_all_scores.csv')\n",
    "# df_m36_all_scores  = pd.read_csv('../Data/data_random_1/Neurocognitive_Joins/df_m36_all_scores.csv')\n",
    "\n",
    "print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "      'there are {} common records for GIA Score'.format(len(df_cluster_join_base_GIA)))\n",
    "print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "      'there are {} common records for Process Speed Score'.format(len(df_cluster_join_base_Proc_Spd)))\n",
    "print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "      'there are {} common records for Working Memory Score'.format(len(df_cluster_join_base_Working_Mem)))\n",
    "print('\\033[1m Among {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "      'there are {} common records for Broad Attention Score\\n'.format(len(df_cluster_join_base_Broad_Attn)))\n",
    "\n",
    "print('\\033[1m Among {} records of Month-36,'.format(len(df_m36)) , 'and {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "      'there are {} common records for GIA Score'.format(len(df_cluster_join_m36base_GIA)))\n",
    "print('\\033[1m Among {} records of Month-36,'.format(len(df_m36)) , 'and {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "      'there are {} common records for Process Speed Score'.format(len(df_cluster_join_m36base_Proc_Spd)))\n",
    "print('\\033[1m Among {} records of Month-36,'.format(len(df_m36)) , 'and {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "      'there are {} common records for Working Memory Score'.format(len(df_cluster_join_m36base_Working_Mem)))\n",
    "print('\\033[1m Among {} records of Month-36,'.format(len(df_m36)) , 'and {} records of Baseline-Phase,'.format(len(df_base)),\n",
    "      'there are {} common records for Broad Attention Score\\n'.format(len(df_cluster_join_m36base_Broad_Attn)))\n",
    "\n",
    "print('\\033[1m There are totally {} common records for all scores in the Baseline'.format(len(df_base_all_scores)))\n",
    "print('\\033[1m There are totally {} common records for all scores in (36months-baseline)'.format(len(df_m36_all_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining of LF_500 with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## LF_500 for Base scores\n",
    "# LF_e500_base_GIA = pd.merge(df_cluster_join_base_GIA, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_base_GIA.index = LF_e500_base_GIA.ID\n",
    "# LF_e500_base_GIA = LF_e500_base_GIA.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_base_GIA.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_base_GIA.csv')\n",
    "\n",
    "# LF_e500_base_Proc_Spd = pd.merge(df_cluster_join_base_Proc_Spd, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_base_Proc_Spd.index = LF_e500_base_Proc_Spd.ID\n",
    "# LF_e500_base_Proc_Spd = LF_e500_base_Proc_Spd.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_base_Proc_Spd.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_base_Proc_Spd.csv')\n",
    "\n",
    "# LF_e500_base_Working_Mem = pd.merge(df_cluster_join_base_Working_Mem, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_base_Working_Mem.index = LF_e500_base_Working_Mem.ID\n",
    "# LF_e500_base_Working_Mem = LF_e500_base_Working_Mem.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_base_Working_Mem.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_base_Working_Mem.csv')\n",
    "\n",
    "# LF_e500_base_Broad_Attn = pd.merge(df_cluster_join_base_Broad_Attn, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_base_Broad_Attn.index = LF_e500_base_Broad_Attn.ID\n",
    "# LF_e500_base_Broad_Attn = LF_e500_base_Broad_Attn.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_base_Broad_Attn.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_base_Broad_Attn.csv')\n",
    "\n",
    "# ##---------------------------------------------------------------------------------\n",
    "# ## LF_500 for (Month36-Base) scores\n",
    "# LF_e500_m36base_GIA = pd.merge(df_cluster_join_m36base_GIA, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_m36base_GIA.index = LF_e500_m36base_GIA.ID\n",
    "# LF_e500_m36base_GIA = LF_e500_m36base_GIA.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_m36base_GIA.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_m36base_GIA.csv')\n",
    "\n",
    "# LF_e500_m36base_Proc_Spd = pd.merge(df_cluster_join_m36base_Proc_Spd, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_m36base_Proc_Spd.index = LF_e500_m36base_Proc_Spd.ID\n",
    "# LF_e500_m36base_Proc_Spd = LF_e500_m36base_Proc_Spd.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_m36base_Proc_Spd.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_m36base_Proc_Spd.csv')\n",
    "\n",
    "# LF_e500_m36base_Working_Mem = pd.merge(df_cluster_join_m36base_Working_Mem, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_m36base_Working_Mem.index = LF_e500_m36base_Working_Mem.ID\n",
    "# LF_e500_m36base_Working_Mem = LF_e500_m36base_Working_Mem.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_m36base_Working_Mem.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_m36base_Working_Mem.csv')\n",
    "\n",
    "# LF_e500_m36base_Broad_Attn = pd.merge(df_cluster_join_m36base_Broad_Attn, LF_500, how='inner', on=['ID'])\n",
    "# LF_e500_m36base_Broad_Attn.index = LF_e500_m36base_Broad_Attn.ID\n",
    "# LF_e500_m36base_Broad_Attn = LF_e500_m36base_Broad_Attn.drop(['ID', 'class_e1500'], axis=1)\n",
    "# # LF_e500_m36base_Broad_Attn.to_csv(r'../Data/data_random_1/LF_Joins/LF_e500_m36base_Broad_Attn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LF_500 for Base scores\n",
    "LF_e500_base_GIA         = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_base_GIA.csv')\n",
    "LF_e500_base_Proc_Spd    = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_base_Proc_Spd.csv')\n",
    "LF_e500_base_Working_Mem = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_base_Working_Mem.csv')\n",
    "LF_e500_base_Broad_Attn  = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_base_Broad_Attn.csv')\n",
    "\n",
    "## LF_500 for (Month36-Base) scores\n",
    "LF_e500_m36base_GIA         = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_m36base_GIA.csv')\n",
    "LF_e500_m36base_Proc_Spd    = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_m36base_Proc_Spd.csv')\n",
    "LF_e500_m36base_Working_Mem = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_m36base_Working_Mem.csv')\n",
    "LF_e500_m36base_Broad_Attn  = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e500_m36base_Broad_Attn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining of LF_1500 with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## LF_1500 for Base scores\n",
    "# LF_e1500_base_GIA = pd.merge(df_cluster_join_base_GIA, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_base_GIA.index = LF_e1500_base_GIA.ID\n",
    "# LF_e1500_base_GIA = LF_e1500_base_GIA.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_base_GIA.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_base_GIA.csv')\n",
    "\n",
    "# LF_e1500_base_Proc_Spd = pd.merge(df_cluster_join_base_Proc_Spd, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_base_Proc_Spd.index = LF_e1500_base_Proc_Spd.ID\n",
    "# LF_e1500_base_Proc_Spd = LF_e1500_base_Proc_Spd.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_base_Proc_Spd.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_base_Proc_Spd.csv')\n",
    "\n",
    "# LF_e1500_base_Working_Mem = pd.merge(df_cluster_join_base_Working_Mem, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_base_Working_Mem.index = LF_e1500_base_Working_Mem.ID\n",
    "# LF_e1500_base_Working_Mem = LF_e1500_base_Working_Mem.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_base_Working_Mem.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_base_Working_Mem.csv')\n",
    "\n",
    "# LF_e1500_base_Broad_Attn = pd.merge(df_cluster_join_base_Broad_Attn, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_base_Broad_Attn.index = LF_e1500_base_Broad_Attn.ID\n",
    "# LF_e1500_base_Broad_Attn = LF_e1500_base_Broad_Attn.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_base_Broad_Attn.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_base_Broad_Attn.csv')\n",
    "\n",
    "# # ##---------------------------------------------------------------------------------\n",
    "# # ## LF_1500 for (Month36-Base) scores\n",
    "# LF_e1500_m36base_GIA = pd.merge(df_cluster_join_m36base_GIA, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_m36base_GIA.index = LF_e1500_m36base_GIA.ID\n",
    "# LF_e1500_m36base_GIA = LF_e1500_m36base_GIA.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_m36base_GIA.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_m36base_GIA.csv')\n",
    "\n",
    "# LF_e1500_m36base_Proc_Spd = pd.merge(df_cluster_join_m36base_Proc_Spd, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_m36base_Proc_Spd.index = LF_e1500_m36base_Proc_Spd.ID\n",
    "# LF_e1500_m36base_Proc_Spd = LF_e1500_m36base_Proc_Spd.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_m36base_Proc_Spd.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_m36base_Proc_Spd.csv')\n",
    "\n",
    "# LF_e1500_m36base_Working_Mem = pd.merge(df_cluster_join_m36base_Working_Mem, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_m36base_Working_Mem.index = LF_e1500_m36base_Working_Mem.ID\n",
    "# LF_e1500_m36base_Working_Mem = LF_e1500_m36base_Working_Mem.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_m36base_Working_Mem.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_m36base_Working_Mem.csv')\n",
    "\n",
    "# LF_e1500_m36base_Broad_Attn = pd.merge(df_cluster_join_m36base_Broad_Attn, LF_1500, how='inner', on=['ID'])\n",
    "# LF_e1500_m36base_Broad_Attn.index = LF_e1500_m36base_Broad_Attn.ID\n",
    "# LF_e1500_m36base_Broad_Attn = LF_e1500_m36base_Broad_Attn.drop(['ID', 'class_e500'], axis=1)\n",
    "# # LF_e1500_m36base_Broad_Attn.to_csv(r'../Data/data_random_1/LF_Joins/LF_e1500_m36base_Broad_Attn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LF_500 for Base scores\n",
    "LF_e1500_base_GIA         = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e1500_base_GIA.csv')\n",
    "LF_e1500_base_Proc_Spd    = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e1500_base_Proc_Spd.csv')\n",
    "LF_e1500_base_Working_Mem = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e1500_base_Working_Mem.csv')\n",
    "LF_e1500_base_Broad_Attn  = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e1500_base_Broad_Attn.csv')\n",
    "\n",
    "## LF_500 for (Month36-Base) scores\n",
    "LF_e1500_m36base_GIA         = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e1500_m36base_GIA.csv')\n",
    "LF_e1500_m36base_Proc_Spd    = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e1500_m36base_Proc_Spd.csv')\n",
    "LF_e1500_m36base_Working_Mem = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e1500_m36base_Working_Mem.csv')\n",
    "LF_e1500_m36base_Broad_Attn  = pd.read_csv('../Data/data_random_1/LF_Joins/LF_e1500_m36base_Broad_Attn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot features\n",
    "\n",
    "bp = dict(linestyle='-', linewidth=2, color='dodgerblue')\n",
    "mp = dict(linestyle='-', linewidth=2, color='r')\n",
    "wp = dict(linestyle='-', linewidth=2, color='dodgerblue')\n",
    "cp = dict(linestyle='-', linewidth=2, color='dodgerblue')\n",
    "fp = dict(markeredgewidth=2, markersize=10, markeredgecolor='dodgerblue')\n",
    "mnp = dict(marker='o', markerfacecolor='lime', markeredgecolor='k', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot: Baseline_GIA\n",
    "\n",
    "## e500\n",
    "boxplot = df_cluster_join_base_GIA.boxplot(column=['GIA_SS'], by=['class_e500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"Baseline Phase\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e500)\", fontsize=15)\n",
    "p.set_ylabel(\"GIA score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## e1500\n",
    "boxplot = df_cluster_join_base_GIA.boxplot(column=['GIA_SS'], by=['class_e1500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"Baseline Phase\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e1500)\", fontsize=15)\n",
    "p.set_ylabel(\"GIA score\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot: Baseline_ProcSpd\n",
    "\n",
    "## e500\n",
    "boxplot = df_cluster_join_base_Proc_Spd.boxplot(column=['Proc_Spd_SS'], by=['class_e500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"Baseline Phase\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e500)\", fontsize=15)\n",
    "p.set_ylabel(\"Process Speed score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## e1500\n",
    "boxplot = df_cluster_join_base_Proc_Spd.boxplot(column=['Proc_Spd_SS'], by=['class_e1500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"Baseline Phase\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e1500)\", fontsize=15)\n",
    "p.set_ylabel(\"Process Speed score\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot: Baseline_WorkingMem\n",
    "\n",
    "## e500\n",
    "boxplot = df_cluster_join_base_Working_Mem.boxplot(column=['Working_Mem_SS'], by=['class_e500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"Baseline Phase\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e500)\", fontsize=15)\n",
    "p.set_ylabel(\"Working Memory score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## e1500\n",
    "boxplot = df_cluster_join_base_Working_Mem.boxplot(column=['Working_Mem_SS'], by=['class_e1500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"Baseline Phase\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e1500)\", fontsize=15)\n",
    "p.set_ylabel(\"Working Memory score\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot: Baseline_BroadAttn\n",
    "\n",
    "## e500\n",
    "boxplot = df_cluster_join_base_Broad_Attn.boxplot(column=['Broad_Attn_SS'], by=['class_e500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"Baseline Phase\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e500)\", fontsize=15)\n",
    "p.set_ylabel(\"Broad Attention score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## e1500\n",
    "boxplot = df_cluster_join_base_Broad_Attn.boxplot(column=['Broad_Attn_SS'], by=['class_e1500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"Baseline Phase\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e1500)\", fontsize=15)\n",
    "p.set_ylabel(\"Broad Attention score\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot features\n",
    "\n",
    "bp = dict(linestyle='-', linewidth=2, color='blueviolet')\n",
    "mp = dict(linestyle='-', linewidth=2, color='r')\n",
    "wp = dict(linestyle='-', linewidth=2, color='blueviolet')\n",
    "cp = dict(linestyle='-', linewidth=2, color='blueviolet')\n",
    "fp = dict(markeredgewidth=2, markersize=10, markeredgecolor='blueviolet')\n",
    "mnp = dict(marker='o', markerfacecolor='lime', markeredgecolor='k', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot: (36months - Baseline)_GIA\n",
    "\n",
    "## e500\n",
    "boxplot = df_cluster_join_m36base_GIA.boxplot(column=['GIA_SS'], by=['class_e500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"(36months - Baseline)\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e500)\", fontsize=15)\n",
    "p.set_ylabel(\"GIA score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## e1500\n",
    "boxplot = df_cluster_join_m36base_GIA.boxplot(column=['GIA_SS'], by=['class_e1500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"(36months - Baseline)\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e1500)\", fontsize=15)\n",
    "p.set_ylabel(\"GIA score\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot: (36months - Baseline)_ProcSpd\n",
    "\n",
    "## e500\n",
    "boxplot = df_cluster_join_m36base_Proc_Spd.boxplot(column=['Proc_Spd_SS'], by=['class_e500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"(36months - Baseline)\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e500)\", fontsize=15)\n",
    "p.set_ylabel(\"Process Speed score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## e1500\n",
    "boxplot = df_cluster_join_m36base_Proc_Spd.boxplot(column=['Proc_Spd_SS'], by=['class_e1500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"(36months - Baseline)\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e1500)\", fontsize=15)\n",
    "p.set_ylabel(\"Process Speed score\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot: (36months - Baseline)_WorkingMem\n",
    "\n",
    "## e500\n",
    "boxplot = df_cluster_join_m36base_Working_Mem.boxplot(column=['Working_Mem_SS'], by=['class_e500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"(36months - Baseline)\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e500)\", fontsize=15)\n",
    "p.set_ylabel(\"Working Memory score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## e1500\n",
    "boxplot = df_cluster_join_m36base_Working_Mem.boxplot(column=['Working_Mem_SS'], by=['class_e1500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"(36months - Baseline)\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e1500)\", fontsize=15)\n",
    "p.set_ylabel(\"Working Memory score\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot: (36months - Baseline)_BroadAttn\n",
    "\n",
    "## e500\n",
    "boxplot = df_cluster_join_m36base_Broad_Attn.boxplot(column=['Broad_Attn_SS'], by=['class_e500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"(36months - Baseline)\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e500)\", fontsize=15)\n",
    "p.set_ylabel(\"Broad Attention score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "## e1500\n",
    "boxplot = df_cluster_join_m36base_Broad_Attn.boxplot(column=['Broad_Attn_SS'], by=['class_e1500'], showfliers=True, showmeans=True, meanprops=mnp,\n",
    "        widths = 0.7, patch_artist=False, grid=False, fontsize=15, boxprops=bp, medianprops=mp, whiskerprops=wp, capprops=cp, flierprops=fp)\n",
    "plt.suptitle(\"\")\n",
    "p = plt.gca()\n",
    "p.set_title(\"(36months - Baseline)\", fontsize=15)\n",
    "p.set_xlabel(\"Classe (e1500)\", fontsize=15)\n",
    "p.set_ylabel(\"Broad Attention score\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
